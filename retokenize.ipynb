{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "38_retokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/38_retokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEfAuk7qz4Vi",
        "outputId": "8d16afd9-aa2a-4a49-8a5a-0a662ebc4761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.attrs import LEMMA\n",
        "from spacy.tokens import Token\n",
        "from spacy.lang.en import English \n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "span = doc[2:4]\n",
        "with doc.retokenize() as retokenizer:\n",
        "    retokenizer.merge(span, attrs={LEMMA: doc.vocab.strings[span.text]})\n",
        "\n",
        "lemma_text = ''.join([token.lemma_ + token.whitespace_ for token in span]).strip()\n",
        "print(lemma_text)\n",
        "\n",
        "def get_lemmas(token):\n",
        "    if token._._lemmas is not None:  # dummy attribute has custom lemmas\n",
        "        return token._._lemmas\n",
        "    return [token.lemma_]  # regular token lemma\n",
        "\n",
        "Token.set_extension('_lemmas', default=None)\n",
        "Token.set_extension('lemmas', getter=get_lemmas)\n",
        "\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "span = doc[2:4]\n",
        "span_lemmas = [token.lemma_ for token in span]  # get list of lemmas\n",
        "span.merge()\n",
        "span[0]._._lemmas = span_lemmas  # write them to dummy attribute of merged token (!)\n",
        "\n",
        "print([token._.lemmas for token in doc])\n",
        "# [['-PRON-'], ['run'], ['3', 'km'], ['yesterday'], ['.']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3km\n",
            "[['I'], ['ran'], ['3', 'km'], ['yesterday'], ['.']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}