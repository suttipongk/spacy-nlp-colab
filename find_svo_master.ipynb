{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_find_SVO_Master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/15_find_SVO_Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BeLcE6XAdx4",
        "outputId": "af7f804a-0603-494e-9989-190e133e8ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set up spaCy\n",
        "import spacy; \n",
        "from spacy.lang.en import English\n",
        "\n",
        "#nlpframework = spacy.load('en_core_web_sm', disable = ['ner', 'parser', 'tagger'])\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "\n",
        "# Test Data\n",
        "multiSentence = \"This is a book.\"\n",
        "parsedData = nlp(multiSentence)\n",
        "for i, token in enumerate(parsedData):\n",
        "    print(\"original:\", token.orth, token.orth_)\n",
        "    print(\"lowercased:\", token.lower, token.lower_)\n",
        "    print(\"lemma:\", token.lemma, token.lemma_)\n",
        "    print(\"shape:\", token.shape, token.shape_)\n",
        "    print(\"prefix:\", token.prefix, token.prefix_)\n",
        "    print(\"suffix:\", token.suffix, token.suffix_)\n",
        "    print(\"log probability:\", token.prob)\n",
        "    print(\"Brown cluster id:\", token.cluster)\n",
        "    print(\"----------------------------------------\")\n",
        "    if i > 10:\n",
        "        break\n",
        "\n",
        "# Let's look at the sentences\n",
        "sents = []\n",
        "for span in parsedData.sents:\n",
        "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
        "    sents.append(sent)\n",
        "\n",
        "for sentence in sents:\n",
        "    print(sentence)\n",
        "\n",
        "for span in parsedData.sents:\n",
        "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
        "    break\n",
        "\n",
        "for token in sent:\n",
        "    print(token.orth_, token.pos_)\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "#from nltk_test.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
        "\n",
        "def getSubsFromConjunctions(subs):\n",
        "    moreSubs = []\n",
        "    for sub in subs:\n",
        "        # rights is a generator\n",
        "        rights = list(sub.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreSubs) > 0:\n",
        "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
        "    return moreSubs\n",
        "\n",
        "def getObjsFromConjunctions(objs):\n",
        "    moreObjs = []\n",
        "    for obj in objs:\n",
        "        # rights is a generator\n",
        "        rights = list(obj.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreObjs) > 0:\n",
        "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
        "    return moreObjs\n",
        "\n",
        "def getVerbsFromConjunctions(verbs):\n",
        "    moreVerbs = []\n",
        "    for verb in verbs:\n",
        "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
        "            if len(moreVerbs) > 0:\n",
        "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
        "    return moreVerbs\n",
        "\n",
        "def findSubs(tok):\n",
        "    head = tok.head\n",
        "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
        "        head = head.head\n",
        "    if head.pos_ == \"VERB\":\n",
        "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
        "        if len(subs) > 0:\n",
        "            verbNegated = isNegated(head)\n",
        "            subs.extend(getSubsFromConjunctions(subs))\n",
        "            return subs, verbNegated\n",
        "        elif head.head != head:\n",
        "            return findSubs(head)\n",
        "    elif head.pos_ == \"NOUN\":\n",
        "        return [head], isNegated(tok)\n",
        "    return [], False\n",
        "\n",
        "def isNegated(tok):\n",
        "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
        "    for dep in list(tok.lefts) + list(tok.rights):\n",
        "        if dep.lower_ in negations:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def findSVs(tokens):\n",
        "    svs = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = getAllSubs(v)\n",
        "        if len(subs) > 0:\n",
        "            for sub in subs:\n",
        "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
        "    return svs\n",
        "\n",
        "def getObjsFromPrepositions(deps):\n",
        "    objs = []\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
        "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
        "    return objs\n",
        "\n",
        "def getObjsFromAttrs(deps):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
        "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
        "            if len(verbs) > 0:\n",
        "                for v in verbs:\n",
        "                    rights = list(v.rights)\n",
        "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "                    objs.extend(getObjsFromPrepositions(rights))\n",
        "                    if len(objs) > 0:\n",
        "                        return v, objs\n",
        "    return None, None\n",
        "\n",
        "def getObjFromXComp(deps):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
        "            v = dep\n",
        "            rights = list(v.rights)\n",
        "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "            objs.extend(getObjsFromPrepositions(rights))\n",
        "            if len(objs) > 0:\n",
        "                return v, objs\n",
        "    return None, None\n",
        "\n",
        "def getAllSubs(v):\n",
        "    verbNegated = isNegated(v)\n",
        "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
        "    if len(subs) > 0:\n",
        "        subs.extend(getSubsFromConjunctions(subs))\n",
        "    else:\n",
        "        foundSubs, verbNegated = findSubs(v)\n",
        "        subs.extend(foundSubs)\n",
        "    return subs, verbNegated\n",
        "\n",
        "def getAllObjs(v):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "    objs.extend(getObjsFromPrepositions(rights))\n",
        "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
        "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "        objs.extend(potentialNewObjs)\n",
        "        v = potentialNewVerb\n",
        "    if len(objs) > 0:\n",
        "        objs.extend(getObjsFromConjunctions(objs))\n",
        "    return v, objs\n",
        "\n",
        "def findSVOs(tokens):\n",
        "    svos = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = getAllSubs(v)\n",
        "        if len(subs) > 0:\n",
        "            v, objs = getAllObjs(v)\n",
        "            for sub in subs:\n",
        "                for obj in objs:\n",
        "                    objNegated = isNegated(obj)\n",
        "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
        "    return svos\n",
        "\n",
        "def getAbuserOntoVictimSVOs(tokens):\n",
        "    maleAbuser = {'he', 'boyfriend', 'bf', 'father', 'dad', 'husband', 'brother', 'man'}\n",
        "    femaleAbuser = {'she', 'girlfriend', 'gf', 'mother', 'mom', 'wife', 'sister', 'woman'}\n",
        "    neutralAbuser = {'pastor', 'abuser', 'offender', 'ex', 'x', 'lover', 'church', 'they'}\n",
        "    victim = {'me', 'sister', 'brother', 'child', 'kid', 'baby', 'friend', 'her', 'him', 'man', 'woman'}\n",
        "\n",
        "    svos = findSVOs(tokens)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    passed = []\n",
        "    for s, v, o in svos:\n",
        "        s = wnl.lemmatize(s)\n",
        "        v = \"!\" + wnl.lemmatize(v[1:], 'v') if v[0] == \"!\" else wnl.lemmatize(v, 'v')\n",
        "        o = \"!\" + wnl.lemmatize(o[1:]) if o[0] == \"!\" else wnl.lemmatize(o)\n",
        "        if s in maleAbuser.union(femaleAbuser).union(neutralAbuser) and o in victim:\n",
        "            passed.append((s, v, o))\n",
        "    return passed\n",
        "\n",
        "def printDeps(toks):\n",
        "    for tok in toks:\n",
        "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
        "\n",
        "# TEST\n",
        "#parse = nlpframework(\"A small company manufactures customized bicycles. Whenever the sales department receives an order, a new process instance is created. A member of the sales department can then reject or accept the order for a customized bike. In the former case, the process instance is finished. In the latter case, the storehouse and the engineering department are informed. The storehouse immediately processes the part list of the order and checks the required quantity of each part. If the part is available in-house, it is reserved. If it is not available, it is back-ordered. This procedure is repeated for each item on the part list. In the meantime, the engineering department prepares everything for the assembling of the ordered bicycle. If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished, the engineering department assembles the bicycle. Afterwards, the sales department ships the bicycle to the customer and finishes the process instance.\")\n",
        "#multiSentence = open('/Users/toppee/Desktop/requirement.txt','r').read().replace('\\n',' ')\n",
        "multiSentence = \"This is a book.\"\n",
        "parse = nlp(multiSentence)\n",
        "print(findSVOs(parse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: 12943039165150086467 This\n",
            "lowercased: 1995909169258310477 this\n",
            "lemma: 1995909169258310477 this\n",
            "shape: 10887629174180191697 Xxxx\n",
            "prefix: 5582244037879929967 T\n",
            "suffix: 2661093235354845946 his\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 3411606890003347522 is\n",
            "lowercased: 3411606890003347522 is\n",
            "lemma: 10382539506755952630 be\n",
            "shape: 4370460163704169311 xx\n",
            "prefix: 5097672513440128799 i\n",
            "suffix: 3411606890003347522 is\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 11901859001352538922 a\n",
            "lowercased: 11901859001352538922 a\n",
            "lemma: 11901859001352538922 a\n",
            "shape: 11123243248953317070 x\n",
            "prefix: 11901859001352538922 a\n",
            "suffix: 11901859001352538922 a\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 13814433107111459297 book\n",
            "lowercased: 13814433107111459297 book\n",
            "lemma: 13814433107111459297 book\n",
            "shape: 13110060611322374290 xxxx\n",
            "prefix: 15598372446745583797 b\n",
            "suffix: 16606746569461509355 ook\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 12646065887601541794 .\n",
            "lowercased: 12646065887601541794 .\n",
            "lemma: 12646065887601541794 .\n",
            "shape: 12646065887601541794 .\n",
            "prefix: 12646065887601541794 .\n",
            "suffix: 12646065887601541794 .\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "This is a book.\n",
            "This DET\n",
            "is AUX\n",
            "a DET\n",
            "book NOUN\n",
            ". PUNCT\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}