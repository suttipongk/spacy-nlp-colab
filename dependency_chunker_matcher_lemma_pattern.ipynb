{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "51_Dependency_Chunker_Matcher_Lemma_Pattern.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/51_Dependency_Chunker_Matcher_Lemma_Pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm-MS-bMl8N6",
        "outputId": "a384a247-505a-4b5b-b7e4-ca2cc90bb943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#-----------------------check dependency-------------------------------\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"\"\"In the context of a claim handling process, \n",
        "it is sometimes necessary to send a questionnaire to the claimant to gather additional information. \n",
        "The claimant is expected to return the questionnaire within five days. \n",
        "If no response is received after five days, a reminder is sent to the claimant. \n",
        "If after another five days there is still no response, another reminder is sent \n",
        "and so on until the completed questionnaire is received.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])\n",
        "    if(token.dep_ == 'xcomp'):\n",
        "      print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In prep is AUX [context]\n",
            "the det context NOUN []\n",
            "context pobj In ADP [the, of]\n",
            "of prep context NOUN [process]\n",
            "a det process NOUN []\n",
            "claim compound process NOUN []\n",
            "handling compound process NOUN []\n",
            "process pobj of ADP [a, claim, handling]\n",
            ", punct is AUX [\n",
            "]\n",
            "\n",
            "  , PUNCT []\n",
            "it nsubj is AUX []\n",
            "is ROOT is AUX [In, ,, it, sometimes, necessary, send, .]\n",
            "sometimes advmod is AUX []\n",
            "necessary acomp is AUX []\n",
            "to aux send VERB []\n",
            "send xcomp is AUX [to, questionnaire, to, gather]\n",
            "send\n",
            "a det questionnaire NOUN []\n",
            "questionnaire dobj send VERB [a]\n",
            "to prep send VERB [claimant]\n",
            "the det claimant NOUN []\n",
            "claimant pobj to ADP [the]\n",
            "to aux gather VERB []\n",
            "gather advcl send VERB [to, information]\n",
            "additional amod information NOUN []\n",
            "information dobj gather VERB [additional]\n",
            ". punct is AUX [\n",
            "]\n",
            "\n",
            "  . PUNCT []\n",
            "The det claimant NOUN []\n",
            "claimant nsubjpass expected VERB [The]\n",
            "is auxpass expected VERB []\n",
            "expected ROOT expected VERB [claimant, is, return, .]\n",
            "to aux return VERB []\n",
            "return xcomp expected VERB [to, questionnaire, within]\n",
            "return\n",
            "the det questionnaire NOUN []\n",
            "questionnaire dobj return VERB [the]\n",
            "within prep return VERB [days]\n",
            "five nummod days NOUN []\n",
            "days pobj within ADP [five]\n",
            ". punct expected VERB [\n",
            "]\n",
            "\n",
            "  . PUNCT []\n",
            "If mark received VERB []\n",
            "no det response NOUN []\n",
            "response nsubjpass received VERB [no]\n",
            "is auxpass received VERB []\n",
            "received advcl sent VERB [If, response, is, after]\n",
            "after prep received VERB [days]\n",
            "five nummod days NOUN []\n",
            "days pobj after ADP [five]\n",
            ", punct sent VERB []\n",
            "a det reminder NOUN []\n",
            "reminder nsubjpass sent VERB [a]\n",
            "is auxpass sent VERB []\n",
            "sent ROOT sent VERB [received, ,, reminder, is, to, .]\n",
            "to prep sent VERB [claimant]\n",
            "the det claimant NOUN []\n",
            "claimant pobj to ADP [the]\n",
            ". punct sent VERB [\n",
            "]\n",
            "\n",
            "  . PUNCT []\n",
            "If mark is AUX []\n",
            "after prep is AUX [days]\n",
            "another det days NOUN []\n",
            "five nummod days NOUN []\n",
            "days pobj after ADP [another, five]\n",
            "there expl is AUX []\n",
            "is advcl sent VERB [If, after, there, still, response]\n",
            "still advmod is AUX []\n",
            "no det response NOUN []\n",
            "response attr is AUX [no]\n",
            ", punct sent VERB []\n",
            "another det reminder NOUN []\n",
            "reminder nsubjpass sent VERB [another]\n",
            "is auxpass sent VERB []\n",
            "sent ROOT sent VERB [is, ,, reminder, is, \n",
            ", and, on, .]\n",
            "\n",
            "  sent VERB []\n",
            "and cc sent VERB []\n",
            "so advmod on ADV []\n",
            "on conj sent VERB [so, received]\n",
            "until mark received VERB []\n",
            "the det questionnaire NOUN []\n",
            "completed amod questionnaire NOUN []\n",
            "questionnaire nsubjpass received VERB [the, completed]\n",
            "is auxpass received VERB []\n",
            "received advcl on ADV [until, questionnaire, is]\n",
            ". punct sent VERB []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3K5rOdmQnv"
      },
      "source": [
        "#----------------------NLTK Chunker------------------------------------\n",
        "import spacy.utils.nltk\n",
        "\n",
        "sentence = \"the little yellow dog barked at the cat\"\n",
        "text = \"\"\"In the context of a claim handling process, \n",
        "it is sometimes necessary to send a questionnaire to the claimant to gather additional information. \n",
        "The claimant is expected to return the questionnaire within five days. \n",
        "If no response is received after five days, a reminder is sent to the claimant. \n",
        "If after another five days there is still no response, another reminder is sent \n",
        "and so on until the completed questionnaire is received.\"\"\"\n",
        "\n",
        "#Define your grammar using regular expressions\n",
        "grammar = ('''\n",
        "    NP: {<DT>?<JJ>*<NN>} # NP\n",
        "    ''')\n",
        "\n",
        "chunkParser = spacy.utils.nltk.RegexpParser(grammar)\n",
        "tagged = spacy.utils.nltk.pos_tag(spacy.utils.nltk.word_tokenize(text))\n",
        "print(tagged)\n",
        "\n",
        "tree = chunkParser.parse(tagged)\n",
        "for subtree in tree.subtrees():\n",
        "    print(subtree)\n",
        "\n",
        "'''\n",
        "(NP the/DT context/NN)\n",
        "(NP a/DT claim/NN)\n",
        "(NP process/NN)\n",
        "(NP a/DT questionnaire/NN)\n",
        "(NP the/DT claimant/NN)\n",
        "(NP additional/JJ information/NN)\n",
        "(NP The/DT claimant/NN)\n",
        "(NP the/DT questionnaire/NN)\n",
        "(NP no/DT response/NN)\n",
        "(NP a/DT reminder/NN)\n",
        "(NP the/DT claimant/NN)\n",
        "(NP no/DT response/NN)\n",
        "(NP another/DT reminder/NN)\n",
        "(NP questionnaire/NN)\n",
        "'''\n",
        "\n",
        "tree.draw()\n",
        "\n",
        "#-------------------Group Grammar-----------------------\n",
        "groucho_grammar = spacy.utils.nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  PP -> P NP\n",
        "  NP -> Det N | Det N PP | 'I'\n",
        "  VP -> V NP | VP PP\n",
        "  Det -> 'an' | 'my'\n",
        "  N -> 'elephant' | 'pajamas'\n",
        "  V -> 'shot'\n",
        "  P -> 'in'\n",
        "\"\"\")\n",
        "\n",
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser = spacy.utils.nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)\n",
        "\n",
        "#--------------------Check with TAG sets----------------\n",
        "chk_set = set(['PRP','MD','NN'])\n",
        "chk_set.issubset(t.tag_ for t in nlp(\"I will go to the mall\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAF5jpkxmdU4",
        "outputId": "753397b6-4803-4ab0-caf1-2a321c75f6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#--------------------Set Custom Boundary----------------\n",
        "import spacy\n",
        "\n",
        "text = \"I want to add a text... field having name as new data\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "print(token.text)\n",
        "print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == \"...\":\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
        "doc = nlp(text)\n",
        "print(\"After:\", [sent.text for sent in doc.sents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "Before: ['I want to add a text...', 'field having name as new data']\n",
            "After: ['I want to add a text...', 'field having name as new data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj_PMGcemnzv",
        "outputId": "6b1376b4-b32a-4053-f84a-52f9715c0521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#--------------------Join Dependency---------------------\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\" \".join(token.tag_ for token in doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NNP VBZ VBG IN VBG NNP NN IN $ CD CD'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKmc6RYamth7"
      },
      "source": [
        "#--------------------Custom Chunk------------------------\n",
        "text = \"\"\"In the context of a claim handling process, \n",
        "it is sometimes necessary to send a questionnaire to the claimant to gather additional information. \n",
        "\"\"\"\n",
        "\n",
        "data = 'The little yellow dog will then walk to the Starbucks, where he will introduce them to Michael.'\n",
        "data_tok = spacy.utils.nltk.word_tokenize(data)       #tokenisation\n",
        "data_pos = spacy.utils.nltk.pos_tag(data_tok)         #POS tagging\n",
        "cfg_1 = \"CUSTOMCHUNK: {<VB><.*>*?<NNP>}\"  #should return `walk to the Starbucks`, etc.\n",
        "chunker = spacy.utils.nltk.RegexpParser(cfg_1)\n",
        "data_chunked = chunker.parse(data_pos)\n",
        "print(data_chunked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKPJVMoBm2WS",
        "outputId": "b0556b91-6154-434a-b360-8adc03e88e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#--------------------Noun chunk---------------------------\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text0 = \"American company listed on NASDAQ in which the Group holds a 23.51% interest as of December 31, 2016.\"\n",
        "text1 = \"Including equity share of refineries in which the Group has a stake.\"\n",
        "text2 = \"Prices for oil and natural gas may fluctuate widely due to many\\nfactors over which TOTAL has no control.\"\n",
        "text3 = \"This\\nscope, which is different from the “operated domain” mentioned\\nabove, includes all the assets in which the Group has a financial\\ninterest or rights to production.\\n \"\n",
        "text4 = \"GHG emissions are also published on an equity interest basis, i.e.,\\nby consolidating the Group share of the emissions of all assets in\\nwhich the Group has a financial interest or rights to production.\\n \"\n",
        "text5 = \"From this profit, minus prior losses, if any, the following items are\\ndeducted in the order indicated:\\n 1) 5% to constitute the legal reserve fund, until said fund reaches\\n10% of the share capital;\\n 2) the amounts set by the Shareholders’ Meeting to fund reserves\\nfor which it determines the allocation or use; and\\n 3) the amounts that the Shareholders’ Meeting decides to retain.\\n \"\n",
        "\n",
        "texts = [text0, text1, text2, text3, text4, text5]\n",
        "\n",
        "for i, t in enumerate(texts):\n",
        "    print('# Noun chunks in text {}:'.format(i))\n",
        "    doc = nlp(t)\n",
        "    for np in doc.noun_chunks:\n",
        "        print(np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Noun chunks in text 0:\n",
            "American company\n",
            "NASDAQ\n",
            "the Group\n",
            "a 23.51% interest\n",
            "December\n",
            "# Noun chunks in text 1:\n",
            "equity share\n",
            "refineries\n",
            "the Group\n",
            "a stake\n",
            "# Noun chunks in text 2:\n",
            "Prices\n",
            "oil\n",
            "natural gas\n",
            "many\n",
            "factors\n",
            "TOTAL\n",
            "no control\n",
            "# Noun chunks in text 3:\n",
            "This\n",
            "scope\n",
            "the “operated domain\n",
            "all the assets\n",
            "the Group\n",
            "a financial\n",
            "interest\n",
            "rights\n",
            "production\n",
            "# Noun chunks in text 4:\n",
            "GHG emissions\n",
            "an equity interest basis\n",
            "the Group share\n",
            "the emissions\n",
            "all assets\n",
            "the Group\n",
            "a financial interest\n",
            "rights\n",
            "production\n",
            "# Noun chunks in text 5:\n",
            "this profit\n",
            "the following items\n",
            "the order\n",
            "the legal reserve fund\n",
            "fund\n",
            "10%\n",
            "the share capital\n",
            "the amounts\n",
            "reserves\n",
            "it\n",
            "the allocation\n",
            "use\n",
            "the amounts\n",
            "the Shareholders’ Meeting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgt-k6Dzm7_Q",
        "outputId": "ac44b7c7-a377-4e0a-b32b-a780dfd2dfb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#--------------------Match String and Number---------------\n",
        "import copy\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add('num_km', None, [{'IS_DIGIT':True}, {'LOWER':'km'}])\n",
        "\n",
        "with doc.retokenize() as retokenizer:\n",
        "    for match_id, start, end in matcher(doc):\n",
        "        retokenizer.merge(doc[start:end], attrs={})\n",
        "        \n",
        "[token.lemma_ for token in doc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-PRON-', 'run', '3km', 'yesterday', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa7REyx6nBy1",
        "outputId": "1f0ca6ca-a15d-409e-92b1-3bea006f17a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#------------------Match String with slice------------------\n",
        "from spacy.attrs import LEMMA\n",
        "\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "span = doc[2:4]\n",
        "with doc.retokenize() as retokenizer:\n",
        "   retokenizer.merge(span, attrs={LEMMA: doc.vocab.strings[span.text]})\n",
        "\n",
        "lemma_text = ''.join([token.lemma_ + token.whitespace_ for token in span]).strip()\n",
        "print(lemma_text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3km\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUKjOHrnJIl",
        "outputId": "a58dcb47-ef1d-4d75-865a-e69c62805b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#------------------Get Lemma--------------------------------\n",
        "from spacy.tokens import Token\n",
        "\n",
        "def get_lemmas(token):\n",
        "  if token._._lemmas is not None:  # dummy attribute has custom lemmas\n",
        "    return token._._lemmas\n",
        "  return [token.lemma_]  # regular token lemma\n",
        "\n",
        "Token.set_extension('_lemmas', default=None)\n",
        "Token.set_extension('lemmas', getter=get_lemmas)\n",
        "\n",
        "doc = nlp(\"I ran 3km yesterday.\")\n",
        "span = doc[2:4]\n",
        "span_lemmas = [token.lemma_ for token in span]  # get list of lemmas\n",
        "span.merge()\n",
        "span[0]._._lemmas = span_lemmas  # write them to dummy attribute of merged token (!)\n",
        "\n",
        "[token._.lemmas for token in doc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['-PRON-'], ['run'], ['3', 'km'], ['yesterday'], ['.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzdw7iANnPdi",
        "outputId": "4a44237c-546c-4ab0-e545-00710ed92e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#-------------------Pattern Group---------------------------\n",
        "import copy\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "color_patterns = [nlp(text) for text in ('red', 'green', 'yellow')]\n",
        "product_patterns = [nlp(text) for text in ('boots', 'coats', 'bag')]\n",
        "material_patterns = [nlp(text) for text in ('silk', 'yellow fabric')]\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add('COLOR', None, *color_patterns)\n",
        "matcher.add('PRODUCT', None, *product_patterns)\n",
        "matcher.add('MATERIAL', None, *material_patterns)\n",
        "\n",
        "doc1 = nlp('yellow fabric')\n",
        "doc2 = nlp('red lipstick and big black boots')\n",
        "\n",
        "for doc in matcher.pipe([doc1, doc2], n_threads=4):\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        rule_id = nlp.vocab.strings[match_id]\n",
        "        span = doc[start : end]\n",
        "        print(rule_id, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COLOR yellow\n",
            "MATERIAL yellow fabric\n",
            "COLOR red\n",
            "PRODUCT boots\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}