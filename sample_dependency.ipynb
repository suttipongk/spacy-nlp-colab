{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/40_sample_dependency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6tFcf8i_Nja",
        "outputId": "bf65576d-0681-4418-ce0a-99be4b6060cf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
            "is be AUX VBZ aux xx True True\n",
            "looking look VERB VBG ROOT xxxx True False\n",
            "at at ADP IN prep xx True True\n",
            "buying buy VERB VBG pcomp xxxx True False\n",
            "U.K. U.K. PROPN NNP compound X.X. False False\n",
            "startup startup NOUN NN dobj xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "$ $ SYM $ quantmod $ False False\n",
            "1 1 NUM CD compound d False False\n",
            "billion billion NUM CD pobj xxxx True False\n",
            "Apple\n",
            "for\n",
            "IN\n",
            "conjunction, subordinating or preposition\n",
            "{96: 2, 87: 1, 100: 2, 85: 2, 92: 1, 99: 1, 93: 2}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.tokens import Span\n",
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "from spacy.kb import KnowledgeBase\n",
        "from spacy.tokens import Span\n",
        "from spacy.matcher import Matcher\n",
        "from __future__ import unicode_literals\n",
        "import spacy,en_core_web_sm\n",
        "from collections import Counter\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "          token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "print(doc[0])\n",
        "print(doc[7])\n",
        "print(doc[7].tag_)\n",
        "print(spacy.explain(doc[7].tag_))\n",
        "\n",
        "#for word in doc:\n",
        "    #print({word.text:{12}}{word.pos_:{10}}{word.tag_:{8}}{spacy.explain(word.tag_)})\n",
        "\n",
        "#word = doc[2]\n",
        "#print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "\n",
        "num_pos = doc.count_by(spacy.attrs.POS)\n",
        "print(num_pos)\n",
        "\n",
        "#for k,v in sorted(num_pos.items()):\n",
        "    #print({k}.{doc.vocab[k].text:{8}}:{v})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "-cZTnpP-_jtA",
        "outputId": "a742ba93-1637-4a0e-b400-0cab29edd890",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3ca295197a16490fad5d4a83c849ecd5-0\" class=\"displacy\" width=\"985\" height=\"264.5\" direction=\"ltr\" style=\"max-width: none; height: 264.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"135\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"135\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"220\">looking</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"220\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"305\">at</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"305\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"390\">buying</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"390\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"475\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"475\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"560\">startup</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"560\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"645\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"645\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"730\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"730\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"815\">1</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"815\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"174.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"900\">billion</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"900\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-0\" stroke-width=\"2px\" d=\"M70,129.5 C70,44.5 215.0,44.5 215.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,131.5 L62,119.5 78,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-1\" stroke-width=\"2px\" d=\"M155,129.5 C155,87.0 210.0,87.0 210.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M155,131.5 L147,119.5 163,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-2\" stroke-width=\"2px\" d=\"M240,129.5 C240,87.0 295.0,87.0 295.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M295.0,131.5 L303.0,119.5 287.0,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-3\" stroke-width=\"2px\" d=\"M325,129.5 C325,87.0 380.0,87.0 380.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M380.0,131.5 L388.0,119.5 372.0,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-4\" stroke-width=\"2px\" d=\"M495,129.5 C495,87.0 550.0,87.0 550.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M495,131.5 L487,119.5 503,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-5\" stroke-width=\"2px\" d=\"M410,129.5 C410,44.5 555.0,44.5 555.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M555.0,131.5 L563.0,119.5 547.0,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-6\" stroke-width=\"2px\" d=\"M410,129.5 C410,2.0 645.0,2.0 645.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M645.0,131.5 L653.0,119.5 637.0,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-7\" stroke-width=\"2px\" d=\"M750,129.5 C750,44.5 895.0,44.5 895.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750,131.5 L742,119.5 758,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-8\" stroke-width=\"2px\" d=\"M835,129.5 C835,87.0 890.0,87.0 890.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M835,131.5 L827,119.5 843,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3ca295197a16490fad5d4a83c849ecd5-0-9\" stroke-width=\"2px\" d=\"M665,129.5 C665,2.0 900.0,2.0 900.0,129.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3ca295197a16490fad5d4a83c849ecd5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M900.0,131.5 L908.0,119.5 892.0,119.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n",
            "Apple - ORG - Companies, agencies, institutions, etc.\n",
            "U.K. - GPE - Countries, cities, states\n",
            "$1 billion - MONEY - Monetary values, including unit\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 85})\n",
        "displacy.serve(doc, style='dep', options={'distance': 120})\n",
        "\n",
        "for entity in doc.ents:\n",
        "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n",
        "\n",
        "ORG = doc.vocab.strings[u'GPE']\n",
        "new_entity = Span(doc, 0, 1, label=ORG)\n",
        "new_entity\n",
        "#doc.ents = list(doc.ents) + [new_entity]\n",
        "#doc.ents\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTERQ9BX_-Ls",
        "outputId": "fcbf6408-840e-46ed-bc30-89c520c1fcdd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       text    lemma    POS  tag       dep\n",
            "0     Apple    Apple  PROPN  NNP     nsubj\n",
            "1        is       be    AUX  VBZ       aux\n",
            "2   looking     look   VERB  VBG      ROOT\n",
            "3        at       at    ADP   IN      prep\n",
            "4    buying      buy   VERB  VBG     pcomp\n",
            "5      U.K.     U.K.  PROPN  NNP  compound\n",
            "6   startup  startup   NOUN   NN      dobj\n",
            "7       for      for    ADP   IN      prep\n",
            "8         $        $    SYM    $  quantmod\n",
            "9         1        1    NUM   CD  compound\n",
            "10  billion  billion    NUM   CD      pobj\n",
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n",
            "Apple True 21.796299 True\n",
            "is True 23.97172 True\n",
            "looking True 23.797874 True\n",
            "at True 21.983685 True\n",
            "buying True 20.953499 True\n",
            "U.K. True 21.744804 True\n",
            "startup True 20.739117 True\n",
            "for True 21.307322 True\n",
            "$ True 22.001581 True\n",
            "1 True 23.677057 True\n",
            "billion True 22.838533 True\n",
            "Apple Apple 1.0\n",
            "Apple is 0.04011939\n",
            "Apple looking -0.15961352\n",
            "Apple at -0.114569284\n",
            "Apple buying -0.0017989352\n",
            "Apple U.K. 0.39125142\n",
            "Apple startup 0.2664353\n",
            "Apple for -0.038983002\n",
            "Apple $ 0.0005354423\n",
            "Apple 1 0.15458857\n",
            "Apple billion 0.115499884\n",
            "is Apple 0.04011939\n",
            "is is 1.0\n",
            "is looking -0.04973421\n",
            "is at 0.19585642\n",
            "is buying 0.017728899\n",
            "is U.K. 0.17062818\n",
            "is startup 0.07659598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is for 0.102153145\n",
            "is $ -0.07032413\n",
            "is 1 0.0275982\n",
            "is billion -0.04126097\n",
            "looking Apple -0.15961352\n",
            "looking is -0.04973421\n",
            "looking looking 1.0\n",
            "looking at 0.20148553\n",
            "looking buying 0.49395216\n",
            "looking U.K. -0.04421929\n",
            "looking startup 0.23787616\n",
            "looking for 0.122941665\n",
            "looking $ 0.14530565\n",
            "looking 1 -0.07630182\n",
            "looking billion 0.13979162\n",
            "at Apple -0.114569284\n",
            "at is 0.19585642\n",
            "at looking 0.20148553\n",
            "at at 1.0\n",
            "at buying 0.08589378\n",
            "at U.K. 0.04348018\n",
            "at startup 0.06360485\n",
            "at for 0.49308112\n",
            "at $ 0.0707303\n",
            "at 1 -0.0076158294\n",
            "at billion 0.11648005\n",
            "buying Apple -0.0017989352\n",
            "buying is 0.017728899\n",
            "buying looking 0.49395216\n",
            "buying at 0.08589378\n",
            "buying buying 1.0\n",
            "buying U.K. 0.07554553\n",
            "buying startup 0.19382489\n",
            "buying for 0.07924722\n",
            "buying $ 0.29554847\n",
            "buying 1 -0.062021594\n",
            "buying billion 0.086430825\n",
            "U.K. Apple 0.39125142\n",
            "U.K. is 0.17062818\n",
            "U.K. looking -0.04421929\n",
            "U.K. at 0.04348018\n",
            "U.K. buying 0.07554553\n",
            "U.K. U.K. 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U.K. startup 0.2233197\n",
            "U.K. for 0.007406217\n",
            "U.K. $ 0.025205925\n",
            "U.K. 1 0.18068945\n",
            "U.K. billion 0.08276237\n",
            "startup Apple 0.2664353\n",
            "startup is 0.07659598\n",
            "startup looking 0.23787616\n",
            "startup at 0.06360485\n",
            "startup buying 0.19382489\n",
            "startup U.K. 0.2233197\n",
            "startup startup 1.0\n",
            "startup for 0.090229705\n",
            "startup $ 0.06714469\n",
            "startup 1 0.15759405\n",
            "startup billion 0.25419807\n",
            "for Apple -0.038983002\n",
            "for is 0.102153145\n",
            "for looking 0.122941665\n",
            "for at 0.49308112\n",
            "for buying 0.07924722\n",
            "for U.K. 0.007406217\n",
            "for startup 0.090229705\n",
            "for for 1.0\n",
            "for $ 0.13778411\n",
            "for 1 0.112874545\n",
            "for billion 0.18372652\n",
            "$ Apple 0.0005354423\n",
            "$ is -0.07032413\n",
            "$ looking 0.14530565\n",
            "$ at 0.0707303\n",
            "$ buying 0.29554847\n",
            "$ U.K. 0.025205925\n",
            "$ startup 0.06714469\n",
            "$ for 0.13778411\n",
            "$ $ 1.0\n",
            "$ 1 0.33198068\n",
            "$ billion 0.1790833\n",
            "1 Apple 0.15458857\n",
            "1 is 0.0275982\n",
            "1 looking -0.07630182\n",
            "1 at -0.0076158294\n",
            "1 buying -0.062021594\n",
            "1 U.K. 0.18068945\n",
            "1 startup 0.15759405\n",
            "1 for 0.112874545\n",
            "1 $ 0.33198068\n",
            "1 1 1.0\n",
            "1 billion 0.6795443\n",
            "billion Apple 0.115499884\n",
            "billion is -0.04126097\n",
            "billion looking 0.13979162\n",
            "billion at 0.11648005\n",
            "billion buying 0.086430825\n",
            "billion U.K. 0.08276237\n",
            "billion startup 0.25419807\n",
            "billion for 0.18372652\n",
            "billion $ 0.1790833\n",
            "billion 1 0.6795443\n",
            "billion billion 1.0\n",
            "Apple 6418411030699964375 Xxxxx A ple True False True en\n",
            "is 3411606890003347522 xx i is True False False en\n",
            "looking 15185124772021413096 xxxx l ing True False False en\n",
            "at 11667289587015813222 xx a at True False False en\n",
            "buying 309587770695063984 xxxx b ing True False False en\n",
            "U.K. 14409890634315022856 X.X. U .K. False False True en\n",
            "startup 7622488711881293715 xxxx s tup True False False en\n",
            "for 16037325823156266367 xxx f for True False False en\n",
            "$ 11283501755624150392 $ $ $ False False False en\n",
            "1 5533571732986600803 d 1 1 False True False en\n",
            "billion 1231493654637052630 xxxx b ion True False False en\n",
            "3197928453018144401\n",
            "coffee\n",
            "coffee\n",
            "coffee\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"tag\", \"dep\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, t.tag_, t.dep_]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "print(df)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "\n",
        "# nlpframework = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
        "for token1 in doc:\n",
        "    for token2 in doc:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))\n",
        "\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)\n",
        "\n",
        "\n",
        "doc = nlp(\"I love coffee\")  # Original Doc\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee' \n",
        "\n",
        "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
        "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
        "\n",
        "empty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\n",
        "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' \n",
        "\n",
        "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
        "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NNM2FoKADf0",
        "outputId": "28f70341-fd51-40cf-cc59-9a772c2036d9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of entities in KB: 3\n",
            "Number of aliases in KB: 2\n"
          ]
        }
      ],
      "source": [
        "# load the model and create an empty KB\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
        "\n",
        "print()\n",
        "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
        "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UinWEZF_BtvC",
        "outputId": "0ad71ee7-d1a9-42a4-a332-8c36453d40c7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n",
            "Peach\n",
            "emoji\n",
            "\n",
            "outranking eggplant\n",
            "Peach emoji\n",
            "Peach is the superior emoji.\n",
            "Fine-grained POS tag PROPN 96\n",
            "Coarse-grained POS tag NNP 15794550382381185553\n",
            "Word shape Xxxxx 16072095006890171862\n",
            "Alphabetic characters? True\n",
            "Punctuation mark? False\n",
            "Digit? False\n",
            "Like a number? True\n",
            "Like an email address? False\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Hello, world. Here are two sentences.\")\n",
        "print([t.text for t in doc])\n",
        "\n",
        "doc = nlp(\"Peach emoji is where it has always been. Peach is the superior \"\n",
        "          \"emoji. It's outranking eggplant  \")\n",
        "print(doc[0].text)          # 'Peach'\n",
        "print(doc[1].text)          # 'emoji'\n",
        "print(doc[-1].text)         # ''\n",
        "print(doc[17:19].text)      # 'outranking eggplant'\n",
        "noun_chunks = list(doc.noun_chunks)\n",
        "print(noun_chunks[0].text)  # 'Peach emoji'\n",
        "\n",
        "sentences = list(doc.sents)\n",
        "assert len(sentences) == 3\n",
        "print(sentences[1].text)    # 'Peach is the superior emoji.'\n",
        "\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "apple = doc[0]\n",
        "print(\"Fine-grained POS tag\", apple.pos_, apple.pos)\n",
        "print(\"Coarse-grained POS tag\", apple.tag_, apple.tag)\n",
        "print(\"Word shape\", apple.shape_, apple.shape)\n",
        "print(\"Alphabetic characters?\", apple.is_alpha)\n",
        "print(\"Punctuation mark?\", apple.is_punct)\n",
        "\n",
        "billion = doc[10]\n",
        "print(\"Digit?\", billion.is_digit)\n",
        "print(\"Like a number?\", billion.like_num)\n",
        "print(\"Like an email address?\", billion.like_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI70C3HqBzKb",
        "outputId": "be56e2de-657e-4c77-9ec8-2a3419a9711a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3197928453018144401 coffee\n",
            "3197928453018144401 3197928453018144401\n",
            "coffee coffee\n",
            "3073001599257881079 beer\n",
            "18234233413267120783 \n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "\n",
        "coffee_hash = nlp.vocab.strings[\"coffee\"]  # 3197928453018144401\n",
        "coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'\n",
        "print(coffee_hash, coffee_text)\n",
        "print(doc[2].orth, coffee_hash)  # 3197928453018144401\n",
        "print(doc[2].text, coffee_text)  # 'coffee'\n",
        "\n",
        "beer_hash = doc.vocab.strings.add(\"beer\")  # 3073001599257881079\n",
        "beer_text = doc.vocab.strings[beer_hash]  # 'beer'\n",
        "print(beer_hash, beer_text)\n",
        "\n",
        "unicorn_hash = doc.vocab.strings.add(\"\")  # 18234233413267120783\n",
        "unicorn_text = doc.vocab.strings[unicorn_hash]  # ''\n",
        "print(unicorn_hash, unicorn_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5PhzlNxB3J6",
        "outputId": "04961c93-84f3-4e33-ebdb-67f637c1e034",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "San Francisco 0 13 GPE\n",
            "FB 0 2 ORG\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "doc = nlp(\"FB is hiring a new VP of global policy\")\n",
        "doc.ents = [Span(doc, 0, 1, label=\"ORG\")]\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvRxYcDmB-MW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_data = [(\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(10):\n",
        "        random.shuffle(train_data)\n",
        "        for text, annotations in train_data:\n",
        "            nlp.update([text], [annotations], sgd=optimizer)\n",
        "nlp.to_disk(\"/model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plEa9e_aCBy3",
        "outputId": "dfdc3d29-75bd-41f9-f07c-e2e2f697c8a3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n",
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n",
            "apple <-> banana 0.47310075\n",
            "pasta <-> hippo 0.36954373\n",
            "True True True True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ],
      "source": [
        "doc_dep = nlp(\"This is a sentence.\")\n",
        "displacy.serve(doc_dep, style=\"dep\")\n",
        "\n",
        "doc_ent = nlp(\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
        "              \"in 2007, few people outside of the company took him seriously.\")\n",
        "displacy.serve(doc_ent, style=\"ent\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
        "apple = doc[0]\n",
        "banana = doc[2]\n",
        "pasta = doc[6]\n",
        "hippo = doc[8]\n",
        "print(\"apple <-> banana\", apple.similarity(banana))\n",
        "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
        "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD4SiZWLCOfh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#from spacy.tokens import Doc\n",
        "#from spacy.vocab import Vocab\n",
        "\n",
        "#nlpframework = spacy.load(\"en_core_web_sm\")\n",
        "#customer_feedback = open(\"customer_feedback_627.txt\").read()\n",
        "#doc = nlpframework(customer_feedback)\n",
        "#doc.to_disk(\"/tmp/customer_feedback_627.bin\")\n",
        "\n",
        "#new_doc = Doc(Vocab()).from_disk(\"/tmp/customer_feedback_627.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-mCETDCCTx4",
        "outputId": "f2cf93dd-51cc-4905-c235-c5c886d88401",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GoogleIO Google I/O\n",
            "HAPPY \n",
            "HAPPY \n",
            "HAPPY \n",
            "Sentiment 0.30000001192092896\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def set_sentiment(matcher, doc, i, matches):\n",
        "    doc.sentiment += 0.1\n",
        "pattern1 = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
        "pattern2 = [[{\"ORTH\": emoji, \"OP\": \"+\"}] for emoji in [\"\", \"\", \"\", \"\"]]\n",
        "matcher.add(\"GoogleIO\", None, pattern1)  # Match \"Google I/O\" or \"Google i/o\"\n",
        "matcher.add(\"HAPPY\", set_sentiment, *pattern2)  # Match one or more happy emoji\n",
        "doc = nlp(\"A text about Google I/O \")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)\n",
        "print(\"Sentiment\", doc.sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb38WXhYC-sm",
        "outputId": "79564885-9def-417a-ed80-87dbf0a0d7f9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['advmod', 'advcl', 'compound', 'nsubj', 'advcl', 'nsubj', 'advcl', 'advcl', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'npadvmod', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'punct', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'advcl', 'pobj', 'prep', 'advcl', 'punct', 'amod', 'nsubj', 'nsubj', 'prep', 'nsubj', 'prep', 'prep', 'nsubj', 'det', 'pobj', 'prep', 'prep', 'nsubj', 'pobj', 'prep', 'prep', 'nsubj', 'dobj', 'advmod', 'punct']\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
        "          \"in 2007, few people outside of the company took him seriously.\")\n",
        "\n",
        "dep_labels = []\n",
        "for token in doc:\n",
        "    while token.head != token:\n",
        "        dep_labels.append(token.dep_)\n",
        "        token = token.head\n",
        "print(dep_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R41eyHHfDCXc",
        "outputId": "95930d79-c1f9-4d21-fa8e-ab70a3732935",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check 8104846059040039827 False\n",
            "out 1696981056005371314 False\n",
            "https://spacy.io 17142293684782158888 True\n",
            "(3, 2)\n",
            "3 2\n",
            "[0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "from spacy.attrs import ORTH, LIKE_URL\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Check out https://spacy.io\")\n",
        "for token in doc:\n",
        "    print(token.text, token.orth, token.like_url)\n",
        "\n",
        "attr_ids = [ORTH, LIKE_URL]\n",
        "doc_array = doc.to_array(attr_ids)\n",
        "print(doc_array.shape)\n",
        "print(len(doc), len(attr_ids))\n",
        "\n",
        "assert doc[0].orth == doc_array[0, 0]\n",
        "assert doc[1].orth == doc_array[1, 0]\n",
        "assert doc[0].like_url == doc_array[0, 1]\n",
        "\n",
        "assert list(doc_array[:, 1]) == [t.like_url for t in doc]\n",
        "print(list(doc_array[:, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezrZmgMaDK0C",
        "outputId": "abf834af-6c51-4997-f9f0-a7f6e7d63645",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pre><span class=\"pos-DET dep-nsubj\">This</span> <span class=\"pos-AUX dep-ROOT\">is</span> <span class=\"pos-DET dep-det\">a</span> <span class=\"pos-NOUN dep-attr\">test</span><span class=\"pos-PUNCT dep-punct\">.</span><span class=\"pos-INTJ dep-intj\">Hello</span>   <span class=\"pos-NOUN dep-ROOT\">world</span><span class=\"pos-PUNCT dep-punct\">.</span></pre>\n"
          ]
        }
      ],
      "source": [
        "def put_spans_around_tokens(doc):\n",
        "    \"\"\"Here, we're building a custom \"syntax highlighter\" for\n",
        "    part-of-speech tags and dependencies. We put each token in a\n",
        "    span element, with the appropriate classes computed. All whitespace is\n",
        "    preserved, outside of the spans. (Of course, HTML will only display\n",
        "    multiple whitespace if enabled  but the point is, no information is lost\n",
        "    and you can calculate what you need, e.g. <br />, <p> etc.)\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    html = '<span class=\"{classes}\">{word}</span>{space}'\n",
        "    for token in doc:\n",
        "        if token.is_space:\n",
        "            output.append(token.text)\n",
        "        else:\n",
        "            classes = \"pos-{} dep-{}\".format(token.pos_, token.dep_)\n",
        "            output.append(html.format(classes=classes, word=token.text, space=token.whitespace_))\n",
        "    string = \"\".join(output)\n",
        "    string = string.replace(\"\\n\", \"\")\n",
        "    string = string.replace(\"\\t\", \"    \")\n",
        "    return \"<pre>{}</pre>\".format(string)\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"This is a test.\\n\\nHello   world.\")\n",
        "html = put_spans_around_tokens(doc)\n",
        "print(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "5k4PCrUaD2fq",
        "outputId": "cee46b02-ee4f-45c7-92eb-45c5bd06ea8a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"en_core_web_sm\")\n",
        "c = Counter(([token.pos_ for token in doc('The cat sat on the mat.')]))\n",
        "sbase = sum(c.values())\n",
        "for el, cnt in c.items():\n",
        "    print(el, '{0:2.2f}%'.format((100.0* cnt)/sbase))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIgxmhlTEGVx",
        "outputId": "23be9d19-565d-4cb2-ac6d-5d0102a5fbce",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The\t0\tthe\tFalse\tFalse\tXxx\tDET\tDT\n",
            "Surveillance\t4\tsurveillance\tFalse\tFalse\tXxxxx\tNOUN\tNN\n",
            "and\t17\tand\tFalse\tFalse\txxx\tCCONJ\tCC\n",
            "Tracking\t21\tTracking\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
            "module\t30\tmodule\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "shall\t37\tshall\tFalse\tFalse\txxxx\tVERB\tMD\n",
            "provide\t43\tprovide\tFalse\tFalse\txxxx\tVERB\tVB\n",
            "the\t51\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
            "system\t55\tsystem\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "administrator\t62\tadministrator\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "with\t76\twith\tFalse\tFalse\txxxx\tADP\tIN\n",
            "the\t81\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
            "ability\t85\tability\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "to\t93\tto\tFalse\tFalse\txx\tPART\tTO\n",
            "monitor\t96\tmonitor\tFalse\tFalse\txxxx\tVERB\tVB\n",
            "system\t104\tsystem\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "configuration\t111\tconfiguration\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "changes\t125\tchange\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
            "posted\t133\tpost\tFalse\tFalse\txxxx\tVERB\tVBN\n",
            "to\t140\tto\tFalse\tFalse\txx\tADP\tIN\n",
            "the\t143\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
            "database\t147\tdatabase\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            ".\t155\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
          ]
        }
      ],
      "source": [
        "#doc = nlpframework(\"Next week I'll   be in Madrid.\")\n",
        "doc = nlp(\"The Surveillance and Tracking module shall provide the system administrator with the ability to monitor system configuration changes posted to the database.\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0kyi9liEKJJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from nltk_test.chunk import conlltags2tree\n",
        "\n",
        "#doc = nlpframework(\"Next week I'll be in Madrid.\")\n",
        "doc = nlp(\"The Surveillance and Tracking module shall provide the system administrator with the ability to monitor system configuration changes posted to the database.\")\n",
        "iob_tagged = [\n",
        "    (\n",
        "        token.text,\n",
        "        token.tag_,\n",
        "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
        "    ) for token in doc\n",
        "]\n",
        "\n",
        "print(iob_tagged)\n",
        "\n",
        "# In case you like the nltk.Tree format\n",
        "print(conlltags2tree(iob_tagged))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "40_sample_dependency.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
