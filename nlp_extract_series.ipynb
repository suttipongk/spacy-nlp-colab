{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_nlp_tutorial_series.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/5_nlp_tutorial_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQNNrUkebbI"
      },
      "source": [
        "import spacy\n",
        "import sys\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "#LOAD MODEL\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#text = \"I can promise it is worth your time.\"\n",
        "#text = \"We can overtake them.\"\n",
        "#text = \"I want a pizza and cola.\"\n",
        "text = \"I want a dish.\"\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ykp7YfehF8",
        "outputId": "4e0e10fa-74c1-4293-d258-e0a2eb2fd6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 1 ================CHECK GET THE Doc=========================\n",
        "text = \"I want a green apple.\"\n",
        "print(w for w in doc[3].lefts)\n",
        "print(w for w in doc[3].children)\n",
        "#print(doc[7].tag_)\n",
        "#print(spacy.explain(doc[7].tag_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object <genexpr> at 0x7fa8d235c048>\n",
            "<generator object <genexpr> at 0x7fa8d235c048>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbHTqU64ewkx"
      },
      "source": [
        "#STEP 2 ================SHOW DETAILS=========================\n",
        "#for word in doc:\n",
        "    #print({word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9DzS2HQe3eM",
        "outputId": "16321ff3-9cfc-4732-ad7f-84f6267820db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 3 ================CONNECT 2 SENTENCE=========================\n",
        "#connection between two sentences\n",
        "for i,sent in enumerate(doc.sents):\n",
        "    print(i)\n",
        "    if(i==0 and sent[0].pos_ == 'PRON'):\n",
        "        print(sent[0].text)\n",
        "        print('The second sentence begins with a pronoun.')\n",
        "\n",
        "counter = 0\n",
        "for sent in doc.sents:\n",
        "    if sent[len(sent)-2].pos_ == 'VERB':\n",
        "        counter += 1\n",
        "    print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "I\n",
            "The second sentence begins with a pronoun.\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW99x36rfM-c",
        "outputId": "3265d0e4-5101-4aeb-ea90-94f51d4dbb7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 4 =============CHUNKS=========================\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(\"chunk :\")\n",
        "    print(chunk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chunk :\n",
            "I\n",
            "chunk :\n",
            "a dish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naXjw6WqfZBt",
        "outputId": "7ea683de-d00f-4451-8b2f-f2c79158a4bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 5 =============COMPOSE DET with ADJ=======================\n",
        "for token in doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        chunk = ''\n",
        "        for w in token.children:\n",
        "            if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
        "                chunk = chunk + w.text + ' '\n",
        "        chunk = chunk + token.text\n",
        "        print(chunk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a dish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7aDkcH2fcu-",
        "outputId": "d1431275-f31c-4b6f-c678-729c00ed3bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 6 =============PRINT SENTENCE=======================\n",
        "text = \"The Golden Gate Bridge is an iconic landmark in San Francisco.\"\n",
        "print([doc[i] for i in range(len(doc))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I, want, a, dish, .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amagWvMgfjpk",
        "outputId": "00f03028-e46d-408c-ac84-0100ed20a2ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 7 =============MERGE WORDING=======================\n",
        "#MERGE wording Golden, Gate, Bridge => Golden Gate Bridge\n",
        "span = doc[1:4]\n",
        "lem_id = doc.vocab.strings[span.text]\n",
        "print(span.merge(lemma = lem_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "want a dish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-ScuvD3gAiU",
        "outputId": "6512fd80-9866-48b4-a44a-ac3832f272cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 8 =============Numeric, Symbolic, and Punctuation Tags=======================\n",
        "text = \"The firm earned $1.5 million in 2017\"\n",
        "#“Numeric, Symbolic, and Punctuation Tags”\n",
        "#for token in doc:\n",
        "    #print({token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {token.dep_:{8}} {spacy.explain(token.pos_)})\n",
        "\n",
        "phrase = ''\n",
        "for token in doc:\n",
        "    if token.tag_ == 'CD':\n",
        "        phrase = token.text\n",
        "        i = token.i+1\n",
        "        while doc[i].tag_ == 'CD':\n",
        "            phrase += doc[i].text + ''\n",
        "            i += 1\n",
        "        break\n",
        "\n",
        "phrase = phrase[:-1]\n",
        "print(phrase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyDaTrnQgUfx",
        "outputId": "113604f3-2d9a-4a17-bfea-5a7ff01d28ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 9 ================“The Use of Pronouns in a Chatbot”===========================\n",
        "sent = ''\n",
        "for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':\n",
        "        sent = doc[i+1].text.capitalize() + ' ' + doc[i].text\n",
        "        sent = sent + ' ' + doc[i+2:].text\n",
        "        break\n",
        "    #By now, you should have : 'Can I promise it is worth your time.'\n",
        "    #Re tokenization\n",
        "\n",
        "for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and token.text == 'I':\n",
        "        sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "        break\n",
        "\n",
        "for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP$' and token.text == 'your':\n",
        "        sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
        "        break\n",
        "\n",
        "for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'VB':\n",
        "        sent = doc[:i].text + ' really ' + doc[i:].text\n",
        "        break\n",
        "\n",
        "sent = doc[:len(doc) - 1].text + '?'\n",
        "print(\"===============sent==================\")\n",
        "print(sent)\n",
        "\n",
        "for token in doc:\n",
        "    if token.dep_ == 'dobj':\n",
        "        shift = len([w for w in token.children])\n",
        "        print(shift)\n",
        "        chunk = doc[i-shift:i+1]\n",
        "        print(chunk)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============sent==================\n",
            "I want a dish?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBeDRs8YgeM6",
        "outputId": "09f8182c-4482-4457-c4ad-1c70705f21e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 10 ================#subject + auxiliary + verb” pattern ==========================\n",
        "#USE DATA text = \"We can overtake them.\"\n",
        "def dep_pattern(doc):\n",
        "    for i in range(len(doc)-1):\n",
        "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and doc[i+2].dep_ == 'ROOT':\n",
        "            for tok in doc[i+2].children:\n",
        "                if tok.dep_ == 'dobj':\n",
        "                    return True\n",
        "                return False\n",
        "\n",
        "if dep_pattern(doc):\n",
        "    print('Found')\n",
        "else:\n",
        "    print('Not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqb8aG7mtOyY"
      },
      "source": [
        "#STEP 11 ==============“Using spaCy’s Matcher to Find Word Sequence Patterns”===================\n",
        "text = \"We can overtake them.\"\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"DEP\":\"nsubj\"},{\"DEP\":\"aux\"},{\"DEP\":\"ROOT\"}]\n",
        "matcher.add(\"NsubjAuxRoot\", None, pattern)\n",
        "#doc = nlpframework(u\"We can overtake them.\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(\"Span: \", span.text)\n",
        "    print(\"The positions in the doc are: \", start, \" - \", end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR92-9Lytgvh",
        "outputId": "d1ea62e9-2b4c-4b5e-81a4-5ba972a5fb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 12 =============pattern: “personal pronoun + modal auxiliary verb + base form verb + . . . + personal pronoun . . .”.”=================\n",
        "text = \"We can overtake them.\"\n",
        "def pos_pattern(doc):\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
        "            return False\n",
        "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
        "            return False\n",
        "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
        "            return False\n",
        "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "#Testing code\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "    print('Found')\n",
        "else:\n",
        "    print('Not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx9RAdGxu2ca",
        "outputId": "cf0f65ee-7db7-4950-88ff-74ffa4742abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 13==============“Creating Patterns Based on Customized Features”====================\n",
        "def pron_pattern(doc):\n",
        "    plural = ['we','us','they','them']\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
        "            if token.text in plural:\n",
        "                return 'plural'\n",
        "            else:\n",
        "                return 'singular'\n",
        "        return 'not found'\n",
        "\n",
        "#การต่่อ String ใช้ (str, str, str)\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "    print('Found:', 'the pronoun in position of direct object is', pron_pattern(doc))\n",
        "else:\n",
        "    print('Not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUCpPJ5GvJex"
      },
      "source": [
        "#STEP 14==============Find Noun by create Pattern List==========================\n",
        "def find_noun(sents, num):\n",
        "    if num == 'plural':\n",
        "        taglist = ['NNS','NNPS']\n",
        "    if num == 'singular':\n",
        "        taglist = ['NN','NNP']\n",
        "\n",
        "    for sent in reversed(sents):\n",
        "        for token in sent:\n",
        "            if token.tag_ in taglist:\n",
        "                return token.text\n",
        "            return 'Noun not found'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWIN0PHRvMxv"
      },
      "source": [
        "#STEP 15============gen_utterance====================\n",
        "def gen_utterance(doc, noun):\n",
        "    sent = ''\n",
        "    for i,token in enumerate(doc):\n",
        "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
        "            sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)]\n",
        "            return sent\n",
        "        return 'Failed to generate an utterance'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-es9aWgIvQv8"
      },
      "source": [
        "#STEP 16\n",
        "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them promptly')\n",
        "sents = list(doc.sents)\n",
        "response = ''\n",
        "noun = ''\n",
        "for i, sent in enumerate(sents):\n",
        "    if dep_pattern(sent) and pos_pattern(sent):\n",
        "        noun = find_noun(sents[:i], pron_pattern(sent))\n",
        "        if noun != 'Noun not found':\n",
        "            response = gen_utterance(sents[i], noun)\n",
        "            break\n",
        "        print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePlEQ9gw-Tx",
        "outputId": "e78716e6-3475-47bb-ced6-e13e4ecfe05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 17============example of premodifiers and postmodifiers===================\n",
        "doc = nlp(u\"Kiwano has jelly-like flesh with a refreshingly fruity taste. This is a nice exotic fruit from Africa. It is definitely worth trying\")\n",
        "fruit_adjectives = []\n",
        "fruit_origins = []\n",
        "for token in doc:\n",
        "    if token.text == 'fruit':\n",
        "        fruit_adjectives = fruit_adjectives + [modifier.text for modifier in token.lefts if modifier.pos_ == 'ADJ']\n",
        "        fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier in token.rights if modifier.text == 'from' and type != 0]\n",
        "        print('The list of adjectival modifiers for word fruit:', fruit_adjectives)\n",
        "        print('The list of GPE names applicable to word fruit as postmodifiers:', fruit_origins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The list of adjectival modifiers for word fruit: ['nice', 'exotic']\n",
            "The list of GPE names applicable to word fruit as postmodifiers: ['Africa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COKe3xtDxDMI",
        "outputId": "18ad1bad-7b51-4cb9-f9ba-ae7f9f50f49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 18===================“Extracting Multiple Intents with token.conjuncts”============================\n",
        "text = nlp(u'I want a pizza and cola.')\n",
        "#Extract direct object and the conjunct associated with it\n",
        "for token in doc:\n",
        "    if token.dep_ == 'dobj':\n",
        "        dobj = [token.text]\n",
        "        conj = [t.text for t in token.conjuncts]\n",
        "        #compose the list of the extracted elements\n",
        "        dobj_conj = dobj + conj\n",
        "        print(dobj_conj)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['flesh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTIyBUC5xPTp"
      },
      "source": [
        "#STEP 19===================“Using Word Lists to Extract the Intent”====================\n",
        "def extract_intent(doc):\n",
        "    dobj = ''\n",
        "    tverb = ''\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'dobj':\n",
        "            dobj = token\n",
        "            tverb = token.head\n",
        "            print(\"HEAD :\", tverb)\n",
        "\n",
        "    #extract the verb for the intent's definition\n",
        "    intentVerb = ''\n",
        "    verbList = ['want', 'like', 'need', 'order']\n",
        "    print('tverb :', tverb.text)\n",
        "    if tverb.text in verbList:\n",
        "        intentVerb = tverb\n",
        "    else:\n",
        "        if tverb.head.dep_ == 'ROOT':\n",
        "            intentVerb = tverb.head\n",
        "\n",
        "    #extract the object for the intent's definition\n",
        "    # USE DATA text = \"I want a pizza and cola.\"\n",
        "    intentObj = ''\n",
        "    objList = ['pizza', 'cola']\n",
        "    if dobj.text in objList:\n",
        "        intentObj = dobj\n",
        "    else:\n",
        "        for child in dobj.children:\n",
        "            if child.dep_ == 'prep':\n",
        "                intentObj = list(child.children)[0]\n",
        "                break\n",
        "            elif child.dep_ == 'compound':\n",
        "                intentObj = child\n",
        "                break\n",
        "    #print the intent expressed in the sample sentence\n",
        "    print(intentVerb.text + intentObj.text.capitalize())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pAZ7VIsxcoe",
        "outputId": "0588ccd7-1d23-4da9-bdca-99c109ddfb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#STEP 20===============“Finding the Meanings of Words Using Synonyms and Semantic Similarity”================\n",
        "# USE DATA text = I want a dish. I'd like to order a pizza. Give me a pie.\n",
        "for token in doc:\n",
        "    verb = ''\n",
        "    dobj = ''\n",
        "\n",
        "    print(\"DOC :\" , token.text)\n",
        "    if token.dep_ == 'dobj':\n",
        "        verb = token.head.text\n",
        "        dobj = token.text\n",
        "        print(\"VERB :\", verb, \"DOBJ :\", dobj)\n",
        "\n",
        "    #create a list of tuples for possible verb synonyms\n",
        "    verbList = [('order', 'want', 'give','make'), ('show','find')]\n",
        "    #find the tuple containing the transitive verb extracted from the sample\n",
        "    verbSyns = [item for item in verbList if verb in item]\n",
        "    #create a list of tuples for possible direct object synonyms\n",
        "    dobjList = [('pizza', 'pie', 'dish'), ('cola', 'soda')]\n",
        "    #find the tuple containing the direct object extracted from the sample\n",
        "    dobjSyns = [item for item in dobjList if dobj in item]\n",
        "    #replace the transitive verb and the direct object with synonyms supported by the application\n",
        "    #and compose the string that represents the intent\n",
        "    #print(verbSyns[0][0])\n",
        "    #print(dobjSyns[0][0].capitalize())\n",
        "    #intent = verbSyns[0][0] + dobjSyns[0][0].capitalize()\n",
        "    #print(intent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DOC : Kiwano\n",
            "DOC : has\n",
            "DOC : jelly\n",
            "DOC : -\n",
            "DOC : like\n",
            "DOC : flesh\n",
            "VERB : has DOBJ : flesh\n",
            "DOC : with\n",
            "DOC : a\n",
            "DOC : refreshingly\n",
            "DOC : fruity\n",
            "DOC : taste\n",
            "DOC : .\n",
            "DOC : This\n",
            "DOC : is\n",
            "DOC : a\n",
            "DOC : nice\n",
            "DOC : exotic\n",
            "DOC : fruit\n",
            "DOC : from\n",
            "DOC : Africa\n",
            "DOC : .\n",
            "DOC : It\n",
            "DOC : is\n",
            "DOC : definitely\n",
            "DOC : worth\n",
            "DOC : trying\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}