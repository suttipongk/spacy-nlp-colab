{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_find_reference_neuralcoref.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/11_find_reference_neuralcoref.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOzAiZNooUAU",
        "outputId": "d7e98890-0dd3-41b9-c2bf-c6aa0075cb21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install spacy==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.8.0)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43zSZOcw5XyT",
        "outputId": "457ce492-8a4e-406d-a858-6d7f2f5bf46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install neuralcoref --no-binary neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/40/8db3db763077fe80b71859f57731261aeb03cc624635f97a3bcfe55ab37b/neuralcoref-4.0.tar.gz (368kB)\n",
            "\r\u001b[K     |▉                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 19.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.18.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7e/3668fb3049a568dead633f8727810e04db95fda9269effdaf8abab56b8fd/boto3-1.16.18-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.0)\n",
            "Collecting botocore<1.20.0,>=1.19.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/e4/3f243b98244f13ac3f7bbe3ec6c4f8473194a200e9785d68696dc5e0d72f/botocore-1.19.18-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 13.5MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.0)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.18->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.18->boto3->neuralcoref) (1.15.0)\n",
            "Skipping wheel build for neuralcoref, due to binaries being disabled for it.\n",
            "\u001b[31mERROR: botocore 1.19.18 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "    Running setup.py install for neuralcoref ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed boto3-1.16.18 botocore-1.19.18 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znNoYYb0o2YH",
        "outputId": "e1a38114-7e4a-4abb-cdc2-7ab883a5cd37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 794kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=5beeb6c69e4ebf508d37911919b9d09087b182720538547fe80ea32516c995f9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mvc2rcqa/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZM1_7_E5Jhh",
        "outputId": "a213ce4d-4bc7-44a1-d736-7f1b4faa149b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "examples = [\n",
        "    u'A small company manufactures customized bicycles.',\n",
        "    u'Whenever the sales department receives an order, a new process instance is created.',\n",
        "    u'A member of the sales department can then reject or accept the order for a customized bike.',\n",
        "    u'In the former case, the process instance is finished.',\n",
        "    u'In the latter case, the storehouse and the engineering department are informed.',\n",
        "    u'The storehouse immediately processes the part list of the order and checks the required quantity of each part.',\n",
        "    u'If the part is available in-house, it is reserved.',\n",
        "    u'If it is not available, it is back-ordered.',\n",
        "    u'In the meantime, the engineering department prepares everything for the assembling of the ordered bicycle.',\n",
        "    u'If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished, the engineering department assembles the bicycle.',\n",
        "    u'Afterwards, the sales department ships the bicycle to the customer and finishes the process instance.',\n",
        "]\n",
        "\n",
        "def printMentions(doc):\n",
        "    for cluster in doc._.coref_clusters:\n",
        "        print (cluster.mentions)\n",
        "\n",
        "def printPronounReferences(doc):\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PRON' and token._.in_coref:\n",
        "            for cluster in token._.coref_clusters:\n",
        "                print (token.text + \" => \" + cluster.main.text)\n",
        "\n",
        "def processDoc(text):\n",
        "    doc = nlp(text)\n",
        "    if doc._.coref_clusters:\n",
        "        printMentions(doc)\n",
        "        printPronounReferences(doc)\n",
        "\n",
        "print(processDoc(examples[6]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[the part is available in-house, it]\n",
            "it => the part is available in-house\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTIOl7Tin-RY",
        "outputId": "b4f7b48a-f69a-4348-aa9f-f0c8f7558e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "doc1 = nlp('My sister has a dog. She loves him.')\n",
        "print(doc1._.coref_clusters)\n",
        "\n",
        "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
        "for ent in doc2.ents:\n",
        "    print(ent._.coref_cluster)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[My sister: [My sister, She], a dog: [a dog, him]]\n",
            "Angela: [Angela, She]\n",
            "Boston: [Boston, that city]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FnEg0IypFBS",
        "outputId": "33da8b6b-3b52-4f62-b546-477dff628528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import re\n",
        "#from nltk_test.tokenize import sent_tokenize   # Add nepural coref to SpaCy's pipe\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp,greedyness=0.5,max_dist=50,blacklist=False)\n",
        "\n",
        "'''\n",
        "text = \"They wore high boots, with their trousers tucked into them, \" \\\n",
        "         \"and had long black hair and heavy black moustaches.\"\n",
        "'''\n",
        "\n",
        "'''\n",
        "text = str(\"The storehouse immediately processes the part list of the order and checks the required quantity of each part. \"\n",
        "+ \" If the part is available in-house, it is reserved. \"\n",
        "+ \" If it is not available, it is back-ordered. \"\n",
        "+ \" This procedure is repeated for each item on the part list.\")\n",
        "'''\n",
        "\n",
        "text = str(\"The storehouse immediately processes the part list of the order and checks the required quantity of each part.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "#การหาความสัมพันธ์ของ Pronoun โดยสร้าง Cluster ของ Coref\n",
        "clusters = doc._.coref_clusters\n",
        "resolved_coref = doc._.coref_resolved\n",
        "\n",
        "#DisplaCy\n",
        "token_data = []\n",
        "for token in doc:\n",
        "  token_row_data = [token.text, token.pos_, token.tag_, token.dep_, (token.head.text, token.head.i), \n",
        "                    token.head.pos_, [child.i for child in token.children]]\n",
        "  token_data.append(token_row_data)\n",
        "token_data_df = pd.DataFrame(token_data, columns=['text','POS','TAG','dependency','head','head_POS','children'])\n",
        "print (token_data_df)\n",
        "\n",
        "#การหาความสัมพันธ์ของ Pronoun\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    if token.pos_ == 'NOUN' and token._.in_coref:\n",
        "        for cluster in token._.coref_clusters:\n",
        "            print (token.text + \" => \" + cluster.main.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           text    POS  TAG  ...             head head_POS               children\n",
            "0           The    DET   DT  ...  (storehouse, 1)     NOUN                     []\n",
            "1    storehouse   NOUN   NN  ...   (processes, 3)     VERB                    [0]\n",
            "2   immediately    ADV   RB  ...   (processes, 3)     VERB                     []\n",
            "3     processes   VERB  VBZ  ...   (processes, 3)     VERB  [1, 2, 6, 10, 11, 18]\n",
            "4           the    DET   DT  ...        (list, 6)     NOUN                     []\n",
            "5          part   NOUN   NN  ...        (list, 6)     NOUN                     []\n",
            "6          list   NOUN   NN  ...   (processes, 3)     VERB              [4, 5, 7]\n",
            "7            of    ADP   IN  ...        (list, 6)     NOUN                    [9]\n",
            "8           the    DET   DT  ...       (order, 9)     NOUN                     []\n",
            "9         order   NOUN   NN  ...          (of, 7)      ADP                    [8]\n",
            "10          and  CCONJ   CC  ...   (processes, 3)     VERB                     []\n",
            "11       checks   VERB  VBZ  ...   (processes, 3)     VERB                   [14]\n",
            "12          the    DET   DT  ...   (quantity, 14)     NOUN                     []\n",
            "13     required   VERB  VBN  ...   (quantity, 14)     NOUN                     []\n",
            "14     quantity   NOUN   NN  ...     (checks, 11)     VERB           [12, 13, 15]\n",
            "15           of    ADP   IN  ...   (quantity, 14)     NOUN                   [17]\n",
            "16         each    DET   DT  ...       (part, 17)     NOUN                     []\n",
            "17         part   NOUN   NN  ...         (of, 15)      ADP                   [16]\n",
            "18            .  PUNCT    .  ...   (processes, 3)     VERB                     []\n",
            "\n",
            "[19 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT0WYpyupjWl",
        "outputId": "96be4fab-b407-47c7-8162-b3412a2cb9b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "nlp =  spacy.load('en_core_web_sm') # create blank Language class\n",
        "coref = neuralcoref.NeuralCoref(nlp.vocab, conv_dict={'Admin': ['man']})\n",
        "nlp.add_pipe(coref, name='neuralcoref')\n",
        "nlp.get_pipe('neuralcoref').set_conv_dict({'Admin': ['man']})\n",
        "#doc=nlp(\"Doctor updated teachers. He modifies locations. Order is also modified by him. Admin creates report. \n",
        "# He also updates report. The Customer steals money. He stole items as well.\")\n",
        "\n",
        "text = \"Whenever a company makes the decision to go public, its first task is to select the underwriters.\" \\\n",
        "\"Underwriters act as financial midwives to a new issue. Usually they play a triple role:\" \\\n",
        "\"First they provide the company with procedural and financial advice, then they buy the issue, and finally they resell it to the public.\" \\\n",
        "\"Established underwriters are careful of their reputation and will not handle a new issue unless they believe the facts have been presented fairly.\" \\\n",
        "\"Thus, in addition to handling the sale of a companys issue, the underwriters in effect give their seal of approval to it.\" \\\n",
        "\"They prepare a registration statement for the approval of the Securities and Exchange Commission (SEC).\" \\\n",
        "\"In addition to registering the issue with the SEC, they need to check that the issue complies with the so-called blue-sky laws of each state that regulate sales of securities within the state.\" \\\n",
        "\"While the registration statement is awaiting approval, underwriters begin to firm up the issue price.\" \\\n",
        "\"They arrange a road show to talk to potential investors. Immediately after they receive clearance from the SEC, underwriters fix the issue price.\" \\\n",
        "\"After that they enter into a firm commitment to buy the stock and then offer it to the public, when they haven’t still found any reason not to do it.\"\n",
        "\n",
        "doc=nlp(text)\n",
        "resolved = doc._.coref_resolved\n",
        "print(doc._.has_coref)\n",
        "print(doc._.coref_clusters)\n",
        "print('resolve---', resolved)\n",
        "print('coref_clusters---', doc._.coref_clusters[0].mentions)\n",
        "\n",
        "#https://github.com/huggingface/neuralcoref/issues/181"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "[a company: [a company, its], the underwriters: [the underwriters, the underwriters, their, They, they], Underwriters: [Underwriters, they, they, they, they], a new issue: [a new issue, the issue, it], Established underwriters: [Established underwriters, their, they], their seal of approval: [their seal of approval, it], a registration statement for the approval of the Securities and Exchange Commission: [a registration statement for the approval of the Securities and Exchange Commission, the registration statement], the SEC: [the SEC, the SEC], underwriters: [underwriters, They, they], the issue price: [the issue price, the issue price], underwriters: [underwriters, they, they], the stock: [the stock, it, it]]\n",
            "resolve--- Whenever a company makes the decision to go public, a company first task is to select the underwriters.Underwriters act as financial midwives to a new issue. Usually Underwriters play a triple role:First Underwriters provide the company with procedural and financial advice, then Underwriters buy a new issue, and finally Underwriters resell a new issue to the public.Established underwriters are careful of Established underwriters reputation and will not handle a new issue unless Established underwriters believe the facts have been presented fairly.Thus, in addition to handling the sale of a companys issue, the underwriters in effect give the underwriters seal of approval to their seal of approval.the underwriters prepare a registration statement for the approval of the Securities and Exchange Commission (SEC).In addition to registering the issue with the SEC, the underwriters need to check that the issue complies with the so-called blue-sky laws of each state that regulate sales of securities within the state.While a registration statement for the approval of the Securities and Exchange Commission is awaiting approval, underwriters begin to firm up the issue price.underwriters arrange a road show to talk to potential investors. Immediately after underwriters receive clearance from the SEC, underwriters fix the issue price.After that underwriters enter into a firm commitment to buy the stock and then offer the stock to the public, when underwriters haven’t still found any reason not to do the stock.\n",
            "coref_clusters--- [a company, its]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}