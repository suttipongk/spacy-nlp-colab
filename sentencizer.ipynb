{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "42_sentencizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/42_sentencizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zFzmspaSS6I",
        "outputId": "bae3de67-4e7c-48f3-c29c-9a2045594bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set up spaCy\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "parser.add_pipe(parser.create_pipe('sentencizer'))\n",
        "\n",
        "# Test Data\n",
        "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
        "                \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
        "                \"In the beginning the Universe was created. This has made a lot of people \" \\\n",
        "                \"very angry and been widely regarded as a bad move.\"\n",
        "\n",
        "# all you have to do to parse text is this:\n",
        "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
        "\n",
        "parsedData = parser(multiSentence)\n",
        "\n",
        "# Let's look at the tokens\n",
        "# All you have to do is iterate through the parsedData\n",
        "# Each token is an object with lots of different properties\n",
        "# A property with an underscore at the end returns the string representation\n",
        "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
        "# The probability estimate is based on counts from a 3 billion word\n",
        "# corpus, smoothed using the Simple Good-Turing method.\n",
        "\n",
        "for i, token in enumerate(parsedData):\n",
        "    print(\"original:\", token.orth, token.orth_)\n",
        "    print(\"lowercased:\", token.lower, token.lower_)\n",
        "    print(\"lemma:\", token.lemma, token.lemma_)\n",
        "    print(\"shape:\", token.shape, token.shape_)\n",
        "    print(\"prefix:\", token.prefix, token.prefix_)\n",
        "    print(\"suffix:\", token.suffix, token.suffix_)\n",
        "    print(\"log probability:\", token.prob)\n",
        "    print(\"Brown cluster id:\", token.cluster)\n",
        "    print(\"----------------------------------------\")\n",
        "    if i > 1:\n",
        "        break\n",
        "\n",
        "# Let's look at the sentences\n",
        "sents = []\n",
        "# the \"sents\" property returns spans\n",
        "# spans have indices into the original string\n",
        "# where each index value represents a token\n",
        "for span in parsedData.sents:\n",
        "    # go from the start to the end of each span, returning each token in the sentence\n",
        "    # combine each token using join()\n",
        "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
        "    sents.append(sent)\n",
        "\n",
        "for sentence in sents:\n",
        "    print(sentence)\n",
        "\n",
        "# Let's look at the part of speech tags of the first sentence\n",
        "for span in parsedData.sents:\n",
        "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
        "    break\n",
        "\n",
        "for token in sent:\n",
        "    print(token.orth_, token.pos_)\n",
        "\n",
        "# Let's look at the dependencies of this example:\n",
        "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
        "parsedEx = parser(example)\n",
        "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
        "for token in parsedEx:\n",
        "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n",
        "\n",
        "# Let's look at the named entities of this example:\n",
        "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
        "parsedEx = parser(example)\n",
        "for token in parsedEx:\n",
        "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
        "\n",
        "print(\"-------------- entities only ---------------\")\n",
        "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
        "ents = list(parsedEx.ents)\n",
        "for entity in ents:\n",
        "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: 6090035477591592277 There\n",
            "lowercased: 2112642640949226496 there\n",
            "lemma: 6090035477591592277 There\n",
            "shape: 16072095006890171862 Xxxxx\n",
            "prefix: 5582244037879929967 T\n",
            "suffix: 18139757808136603089 ere\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 3411606890003347522 is\n",
            "lowercased: 3411606890003347522 is\n",
            "lemma: 3411606890003347522 is\n",
            "shape: 4370460163704169311 xx\n",
            "prefix: 5097672513440128799 i\n",
            "suffix: 3411606890003347522 is\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "original: 15099054000809333061 an\n",
            "lowercased: 15099054000809333061 an\n",
            "lemma: 15099054000809333061 an\n",
            "shape: 4370460163704169311 xx\n",
            "prefix: 11901859001352538922 a\n",
            "suffix: 15099054000809333061 an\n",
            "log probability: -20.0\n",
            "Brown cluster id: 0\n",
            "----------------------------------------\n",
            "There is an art, it says, or rather, a knack to flying.\n",
            "The knack lies in learning how to throw yourself at the ground and miss.\n",
            "In the beginning the Universe was created.\n",
            "This has made a lot of people very angry and been widely regarded as a bad move.\n",
            "There \n",
            "is \n",
            "an \n",
            "art \n",
            ", \n",
            "it \n",
            "says \n",
            ", \n",
            "or \n",
            "rather \n",
            ", \n",
            "a \n",
            "knack \n",
            "to \n",
            "flying \n",
            ". \n",
            "The  The [] []\n",
            "boy  boy [] []\n",
            "with  with [] []\n",
            "the  the [] []\n",
            "spotted  spotted [] []\n",
            "dog  dog [] []\n",
            "quickly  quickly [] []\n",
            "ran  ran [] []\n",
            "after  after [] []\n",
            "the  the [] []\n",
            "firetruck  firetruck [] []\n",
            ".  . [] []\n",
            "Apple (not an entity)\n",
            "'s (not an entity)\n",
            "stocks (not an entity)\n",
            "dropped (not an entity)\n",
            "dramatically (not an entity)\n",
            "after (not an entity)\n",
            "the (not an entity)\n",
            "death (not an entity)\n",
            "of (not an entity)\n",
            "Steve (not an entity)\n",
            "Jobs (not an entity)\n",
            "in (not an entity)\n",
            "October (not an entity)\n",
            ". (not an entity)\n",
            "-------------- entities only ---------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}