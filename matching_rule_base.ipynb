{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "28_matching_rule_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/28_matching_rule_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMQQyJcGcLSA",
        "outputId": "f6e474cf-71fa-4fd4-b0c7-8f1b25421f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Token\n",
        "from spacy.tokens import Span\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy.pipeline import merge_entities\n",
        "from spacy import displacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
        "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
        "matcher.add(\"HelloWorld\", None, pattern)\n",
        "\n",
        "doc = nlp(\"Hello, world! Hello world!\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15578876784678163569 HelloWorld 0 3 Hello, world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0uohSxDcPUv",
        "outputId": "e5351be0-b043-40e0-eb19-87d1393bef39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
        "\n",
        "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
        "for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end)\n",
        "    # This is a Span object or None if match doesn't map to valid token sequence\n",
        "    if span is not None:\n",
        "        print(\"Found match:\", span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found match: United States\n",
            "Found match: United States\n",
            "Found match: U.S.\n",
            "Found match: US\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZM4q574c4NN"
      },
      "source": [
        "matcher = Matcher(nlp.vocab, validate=True)\n",
        "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\n",
        "#matcher.add(\"HelloWorld\", None, pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJMNpU4QdE2y",
        "outputId": "7b89ae11-c935-4896-e0a5-757f90941e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def add_event_ent(matcher, doc, i, matches):\n",
        "    # Get the current match and create tuple of entity label, start and end.\n",
        "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
        "    match_id, start, end = matches[i]\n",
        "    entity = Span(doc, start, end, label=\"EVENT\")\n",
        "    doc.ents += (entity,)\n",
        "    print(entity.text)\n",
        "\n",
        "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
        "matcher.add(\"GoogleIO\", add_event_ent, pattern)\n",
        "doc = nlp(\"This is a text about Google I/O\")\n",
        "matches = matcher(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google I/O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBxlARKqdIv9",
        "outputId": "07352385-7c9c-4a86-f7fb-a5d12faf6921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#================================================\n",
        "# We're using a class because the component needs to be initialised with\n",
        "# the shared vocab via the nlpframework object\n",
        "class BadHTMLMerger(object):\n",
        "    def __init__(self, nlp):\n",
        "        # Register a new token extension to flag bad HTML\n",
        "        Token.set_extension(\"bad_html\", default=False)\n",
        "        self.matcher = Matcher(nlp.vocab)\n",
        "        self.matcher.add(\n",
        "            \"BAD_HTML\",\n",
        "            None,\n",
        "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br\"}, {\"ORTH\": \">\"}],\n",
        "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br/\"}, {\"ORTH\": \">\"}],\n",
        "        )\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        # This method is invoked when the component is called on a Doc\n",
        "        matches = self.matcher(doc)\n",
        "        spans = []  # Collect the matched spans here\n",
        "        for match_id, start, end in matches:\n",
        "            spans.append(doc[start:end])\n",
        "        with doc.retokenize() as retokenizer:\n",
        "            for span in spans:\n",
        "                retokenizer.merge(span)\n",
        "                for token in span:\n",
        "                    token._.bad_html = True  # Mark token as bad HTML\n",
        "        return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "html_merger = BadHTMLMerger(nlp)\n",
        "nlp.add_pipe(html_merger, last=True)  # Add component to the pipeline\n",
        "doc = nlp(\"Hello<br>world! <br/> This is a test.\")\n",
        "for token in doc:\n",
        "    print(token.text, token._.bad_html)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello False\n",
            "<br> True\n",
            "world False\n",
            "! False\n",
            "<br/> True\n",
            "This False\n",
            "is False\n",
            "a False\n",
            "test False\n",
            ". False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ckykfTdNjd",
        "outputId": "c07c7912-72b3-41b6-ac2a-e1c666d4ffd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matched_sents = []  # Collect data of matched sentences to be visualized\n",
        "\n",
        "def collect_sents(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    span = doc[start:end]  # Matched span\n",
        "    sent = span.sent  # Sentence containing matched span\n",
        "    # Append mock entity for match in displaCy style to matched_sents\n",
        "    # get the match span by ofsetting the start and end of the span with the\n",
        "    # start and end of the sentence in the doc\n",
        "    match_ents = [{\n",
        "        \"start\": span.start_char - sent.start_char,\n",
        "        \"end\": span.end_char - sent.start_char,\n",
        "        \"label\": \"MATCH\",\n",
        "    }]\n",
        "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
        "\n",
        "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
        "           {\"POS\": \"ADJ\"}]\n",
        "matcher.add(\"FacebookIs\", collect_sents, pattern)  # add pattern\n",
        "doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Serve visualization of sentences containing match with displaCy\n",
        "# set manual=True to make displaCy render straight from a dictionary\n",
        "# (if you're not running the code within a Jupyer environment, you can\n",
        "# use displacy.serve instead)\n",
        "displacy.render(matched_sents, style=\"ent\", manual=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I\\'d say that \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Facebook is evil\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\\n</mark>\\n.</div>\\n\\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">– \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Facebook is pretty cool\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\\n</mark>\\n, right?</div>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJKRsSWMdSlF",
        "outputId": "2139b244-4037-492e-fa35-b9497ed14285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
        "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"ddd\"}]\n",
        "matcher.add(\"PHONE_NUMBER\", None, pattern)\n",
        "\n",
        "doc = nlp(\"Call me at (123) 456 789 or (123) 456 789!\")\n",
        "print([t.text for t in doc])\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Call', 'me', 'at', '(', '123', ')', '456', '789', 'or', '(', '123', ')', '456', '789', '!']\n",
            "(123) 456 789\n",
            "(123) 456 789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHJIEKxidWBH",
        "outputId": "c92bfe22-c2e9-4d54-d95e-77074c923123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
        "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
        "\n",
        "# Add patterns to match one or more emoji tokens\n",
        "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
        "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
        "\n",
        "# Function to label the sentiment\n",
        "def label_sentiment(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
        "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
        "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
        "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
        "\n",
        "matcher.add(\"HAPPY\", label_sentiment, *pos_patterns)  # Add positive pattern\n",
        "matcher.add(\"SAD\", label_sentiment, *neg_patterns)  # Add negative pattern\n",
        "\n",
        "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
        "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n",
        "\n",
        "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAPPY 😀\n",
            "HASHTAG #MondayMotivation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wE11V6bdZBm",
        "outputId": "89039ff7-81c3-4a43-d0e4-27fa14e66376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
        "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n",
        "\n",
        "# Register token extension\n",
        "Token.set_extension(\"is_hashtag\", default=False)\n",
        "\n",
        "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
        "matches = matcher(doc)\n",
        "hashtags = []\n",
        "for match_id, start, end in matches:\n",
        "    if doc.vocab.strings[match_id] == \"HASHTAG\":\n",
        "        hashtags.append(doc[start:end])\n",
        "with doc.retokenize() as retokenizer:\n",
        "    for span in hashtags:\n",
        "        retokenizer.merge(span)\n",
        "        for token in span:\n",
        "            token._.is_hashtag = True\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token._.is_hashtag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello False\n",
            "world False\n",
            "😀 False\n",
            "#MondayMotivation True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzVKcUpbdu2J",
        "outputId": "3dab6eeb-9120-4f83-dcdf-daf337703b60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
        "# Only run nlpframework.make_doc to speed things up\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", None, *patterns)\n",
        "\n",
        "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
        "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Angela Merkel\n",
            "Barack Obama\n",
            "Washington, D.C.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COEbhAc3dxgA",
        "outputId": "a2ad7285-da7e-4a8b-d583-3ecf2ecca67a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
        "matcher.add(\"Names\", None, *patterns)\n",
        "\n",
        "doc = nlp(\"angela merkel and us president barack Obama\")\n",
        "for match_id, start, end in matcher(doc):\n",
        "    print(\"Matched based on lowercase token text:\", doc[start:end])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched based on lowercase token text: angela merkel\n",
            "Matched based on lowercase token text: barack Obama\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoSw6pOneQYi",
        "outputId": "c3566467-3529-466f-fe3b-abf96c6f7d11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
        "matcher.add(\"IP\", None, nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\"))\n",
        "\n",
        "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
        "for match_id, start, end in matcher(doc):\n",
        "    print(\"Matched based on token shape:\", doc[start:end])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched based on token shape: 192.168.1.1\n",
            "Matched based on token shape: 192.168.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWUtPQr2eUDu",
        "outputId": "dc8db4fb-d3ec-4788-b1b2-d8a3fffeedb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
        "matcher.add(\"IP\", None, nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\"))\n",
        "\n",
        "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
        "for match_id, start, end in matcher(doc):\n",
        "    print(\"Matched based on token shape:\", doc[start:end])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched based on token shape: 192.168.1.1\n",
            "Matched based on token shape: 192.168.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqy-vvFegYb",
        "outputId": "deae8dee-b367-482a-d649-119a9a084a33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "ruler = EntityRuler(nlp)\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.add_pipe(ruler)\n",
        "\n",
        "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Apple', 'ORG'), ('San Francisco', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTf8g8hmeixs",
        "outputId": "727219ec-1d69-4f0e-c39f-8003d504af92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ruler = EntityRuler(nlp)\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.add_pipe(ruler)\n",
        "\n",
        "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('MyCorp Inc.', 'ORG'), ('U.S.', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBJ9045elx5",
        "outputId": "d9cf0df5-0cbd-48e1-d07f-e3be029632b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = English()\n",
        "ruler = EntityRuler(nlp)\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.add_pipe(ruler)\n",
        "\n",
        "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
        "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
        "\n",
        "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
        "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Apple', 'ORG', 'apple'), ('San Francisco', 'GPE', 'san-francisco')]\n",
            "[('Apple', 'ORG', 'apple'), ('San Fran', 'GPE', 'san-francisco')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1EFWAn-ep4v",
        "outputId": "3fb7d993-5312-4b5c-c75a-141cce98b42c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def expand_person_entities(doc):\n",
        "    new_ents = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
        "            prev_token = doc[ent.start - 1]\n",
        "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
        "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
        "                new_ents.append(new_ent)\n",
        "        else:\n",
        "            new_ents.append(ent)\n",
        "    doc.ents = new_ents\n",
        "    return doc\n",
        "\n",
        "# Add the component after the named entity recognizer\n",
        "nlp.add_pipe(expand_person_entities, after='ner')\n",
        "\n",
        "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Dr Alex Smith', 'PERSON'), ('Acme Corp Inc.', 'ORG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34aE2f_ewht",
        "outputId": "7ba52696-3b9d-486c-98dd-61e5173275c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_person_title(span):\n",
        "    if span.label_ == \"PERSON\" and span.start != 0:\n",
        "        prev_token = span.doc[span.start - 1]\n",
        "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
        "            return prev_token.text\n",
        "\n",
        "# Register the Span extension as 'person_title'\n",
        "Span.set_extension(\"person_title\", getter=get_person_title)\n",
        "\n",
        "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
        "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Alex Smith', 'PERSON', 'Dr'), ('Acme Corp Inc.', 'ORG', None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aFF98oYfArr",
        "outputId": "2fbccdba-1319-4a36-e08b-36b08f64c5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_person_orgs(doc):\n",
        "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    for ent in person_entities:\n",
        "        head = ent.root.head\n",
        "        if head.lemma_ == \"work\":\n",
        "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
        "            for prep in preps:\n",
        "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
        "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
        "    return doc\n",
        "\n",
        "# To make the entities easier to work with, we'll merge them into single tokens\n",
        "nlp.add_pipe(merge_entities)\n",
        "nlp.add_pipe(extract_person_orgs)\n",
        "\n",
        "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
        "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
        "displacy.render(doc, options={'fine_grained': True})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5f012be5fd1d4760ba5b4d32c4fdc186-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\\n</text>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\\n</g>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\\n</g>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-5f012be5fd1d4760ba5b4d32c4fdc186-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\\n</g>\\n</svg>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMz0KZyufaBX"
      },
      "source": [
        "def extract_person_orgs(doc):\n",
        "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    for ent in person_entities:\n",
        "        head = ent.root.head\n",
        "        if head.lemma_ == \"work\":\n",
        "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
        "            for prep in preps:\n",
        "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
        "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
        "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
        "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
        "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Del0JSw9ffj4",
        "outputId": "5ef9be35-c588-441c-81cf-21d074741aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load a model and create the nlpframework object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "matcher.add('IPHONE_PATTERN', None, pattern)\n",
        "\n",
        "# Process some text\n",
        "doc = nlp(\"New iPhone X release date leaked\")\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwD6gf2kgMIN",
        "outputId": "772e61d8-20be-4251-9d32-92acab989621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "from spacy.pipeline import merge_entities\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "### {highlight=\"9-11\"}\n",
        "'''\n",
        "def extract_person_orgs(doc):\n",
        "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    for ent in person_entities:\n",
        "        head = ent.root.head\n",
        "        if head.lemma_ == \"work\":\n",
        "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
        "            for prep in preps:\n",
        "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
        "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
        "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
        "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
        "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
        "    return doc\n",
        "'''\n",
        "\n",
        "def extract_person_orgs(doc):\n",
        "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    for ent in person_entities:\n",
        "        head = ent.root.head\n",
        "        if head.lemma_ == \"work\":\n",
        "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
        "            for prep in preps:\n",
        "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
        "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
        "    return doc\n",
        "\n",
        "# To make the entities easier to work with, we'll merge them into single tokens\n",
        "nlp.add_pipe(merge_entities)\n",
        "nlp.add_pipe(extract_person_orgs)\n",
        "\n",
        "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
        "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
        "#displacy.render(doc, options={'fine_grained': True})\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n",
        "\n",
        "#Match exact token texts\n",
        "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
        "#Match lexical attributes\n",
        "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
        "#Match any token attributes\n",
        "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def add_event_ent(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    entity = Span(doc, start, end, label=\"EVENT\")\n",
        "    doc.ents += (entity,)\n",
        "    print(entity.text)\n",
        "\n",
        "#Matching lexical attributes\n",
        "pattern = [\n",
        "    {'IS_DIGIT': True},     #2018\n",
        "    {'LOWER': 'fifa'},      #fifa\n",
        "    {'LOWER': 'world'},     #world\n",
        "    {'LOWER': 'cup'},       #cup\n",
        "    {'IS_PUNCT': True}      #:\n",
        "]\n",
        "\n",
        "matcher.add(\"fifa\", add_event_ent, pattern)\n",
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bc7c428de3484ab7b02e69b573864d82-0\" class=\"displacy\" width=\"410\" height=\"182.0\" direction=\"ltr\" style=\"max-width: none; height: 182.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">worked</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">at</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">Acme Corp Inc.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bc7c428de3484ab7b02e69b573864d82-0-0\" stroke-width=\"2px\" d=\"M70,47.0 C70,2.0 140.0,2.0 140.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bc7c428de3484ab7b02e69b573864d82-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,49.0 L62,37.0 78,37.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bc7c428de3484ab7b02e69b573864d82-0-1\" stroke-width=\"2px\" d=\"M160,47.0 C160,2.0 230.0,2.0 230.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bc7c428de3484ab7b02e69b573864d82-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M230.0,49.0 L238.0,37.0 222.0,37.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bc7c428de3484ab7b02e69b573864d82-0-2\" stroke-width=\"2px\" d=\"M250,47.0 C250,2.0 320.0,2.0 320.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bc7c428de3484ab7b02e69b573864d82-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M320.0,49.0 L328.0,37.0 312.0,37.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2018 FIFA World Cup:\n",
            "2018 FIFA World Cup:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJp8WZ98gRVU",
        "outputId": "8f66953b-86b8-47a0-e280-b9e31adb216b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{'LOWER': 'computer', 'POS': 'NOUN'},\n",
        "           {'POS':{'NOT_IN': ['VERB']}}]\n",
        "\n",
        "matcher.add(\"Matching\", None, pattern)\n",
        "doc = nlp(\"Computer programming is the process of writing instructions that get executed by computers. The instructions, also known as code, are written in a programming language which the computer can understand and use to perform a task or solve a problem. Basic computer programming involves the analysis of a problem and development of a logical sequence of instructions to solve it. There can be numerous paths to a solution and the computer programmer seeks to design and code that which is most efficient. Among the programmer’s tasks are understanding requirements, determining the right programming language to use, designing or architecting the solution, coding, testing, debugging and writing documentation so that the solution can be easily understood by other programmers.Computer programming is at the heart of computer science. It is the implementation portion of software development, application development and software engineering efforts, transforming ideas and theories into actual, working solutions.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6895354335150655416 Matching 0 2 Computer programming\n",
            "6895354335150655416 Matching 45 47 computer programming\n",
            "6895354335150655416 Matching 75 77 computer programmer\n",
            "6895354335150655416 Matching 131 133 Computer programming\n",
            "6895354335150655416 Matching 138 140 computer science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrNnCMyagWJX",
        "outputId": "feb04b38-3553-49f2-9405-12a3133939f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
        "    {'POS': 'NOUN'}\n",
        "]\n",
        "\n",
        "matcher.add(\"Matching\", None, pattern)\n",
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6895354335150655416 Matching 1 3 loved dogs\n",
            "6895354335150655416 Matching 6 8 love cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MvpvDlUgZee",
        "outputId": "c1e72a1c-fb86-4a4d-ec9d-114c33a63740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {'LEMMA': 'buy'},\n",
        "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
        "    {'POS': 'NOUN'}\n",
        "]\n",
        "\n",
        "matcher.add(\"Matching\", None, pattern)\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    # Get the string representation\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)\n",
        "#https://github.com/ines/spacy-course/tree/master/slides"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6895354335150655416 Matching 1 4 bought a smartphone\n",
            "6895354335150655416 Matching 8 10 buying apps\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}