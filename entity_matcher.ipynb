{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_entity_matcher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/6_entity_matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtnMGgax-pL",
        "outputId": "ad9228ea-d6f2-4494-a022-6fe2b5273d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text= \" Each morning, the files which have yet to be processed need to be checked, \"\n",
        "\" to make sure they are in order for the court hearing that day. If some files are missing, \"\n",
        "\" a search is initiated, otherwise the files can be physically tracked to the intended location. \"\n",
        "\" Once all the files are ready, these are handed to the Associate, and meantime the Judges Lawlist is distributed to the relevant people. \"\n",
        "\" Afterwards, the directions hearings are conducted.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "\n",
        "print(sents_list)\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' Each morning, the files which have yet to be processed need to be checked,']\n",
            "[' ', 'Each', 'morning', ',', 'the', 'files', 'which', 'have', 'yet', 'to', 'be', 'processed', 'need', 'to', 'be', 'checked', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfi9WnzAyHif",
        "outputId": "f23b22e0-d7b3-4460-d5a0-5c2b75bbc2df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
            "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1C0UIAHyLhw",
        "outputId": "96141c60-c324-4856-9ee5-1d493e4d3085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def my_component(doc):\n",
        "    print(\"After tokenization, this doc has {} tokens.\".format(len(doc)))\n",
        "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
        "    if len(doc) < 10:\n",
        "        print(\"This is a pretty short document.\")\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(my_component, name=\"print_info\", last=True)\n",
        "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
        "doc = nlp(\"This is a sentence.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'print_info']\n",
            "After tokenization, this doc has 5 tokens.\n",
            "The part-of-speech tags are: ['DET', 'AUX', 'DET', 'NOUN', 'PUNCT']\n",
            "This is a pretty short document.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmgP2WWCyQID",
        "outputId": "031a08af-cf8f-48c5-acf3-aecc0afe8e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "class EntityMatcher(object):\n",
        "    name = \"entity_matcher\"\n",
        "    def __init__(self, nlp, terms, label):\n",
        "        patterns = [nlp.make_doc(text) for text in terms]\n",
        "        self.matcher = PhraseMatcher(nlp.vocab)\n",
        "        self.matcher.add(label, None, *patterns)\n",
        "    def __call__(self, doc):\n",
        "        matches = self.matcher(doc)\n",
        "        for match_id, start, end in matches:\n",
        "            span = Span(doc, start, end, label=match_id)\n",
        "            doc.ents = list(doc.ents) + [span]\n",
        "        return doc\n",
        "        \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "terms = (\"cat\", \"dog\", \"tree kangaroo\", \"giant sea spider\")\n",
        "entity_matcher = EntityMatcher(nlp, terms, \"ANIMAL\")\n",
        "nlp.add_pipe(entity_matcher, after=\"ner\")\n",
        "print(nlp.pipe_names)  # The components in the pipeline\n",
        "doc = nlp(\"This is a text about Barack Obama and a tree kangaroo\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'entity_matcher']\n",
            "[('Barack Obama', 'LOC'), ('tree kangaroo', 'ANIMAL')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYfjqr36yXYy",
        "outputId": "50bcd451-51d9-41a0-9748-dc163276ae0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"|\" and doc[i+1].is_title:\n",
        "            doc[i+1].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(custom_sentencizer, before=\"parser\")  # Insert before the parser\n",
        "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is. A sentence. |\n",
            "This is. Another sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xjOe-cye0S",
        "outputId": "0b6868da-aa01-49b9-f05b-eaf550c60b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "\"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
        "based on list of single or multiple-word company names. Companies are\n",
        "labelled as ORG and their spans are merged into one token. Additionally,\n",
        "._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
        "respectively.\n",
        "\n",
        "* Custom pipeline components: https://spacy.io//usage/processing-pipelines#custom-components\n",
        "\n",
        "Compatible with: spaCy v2.0.0+\n",
        "Last tested with: v2.1.0\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    text=(\"Text to process\", \"positional\", None, str),\n",
        "    companies=(\"Names of technology companies\", \"positional\", None, str),\n",
        ")\n",
        "def main(text=\"Alphabet Inc. is the company behind Google.\", *companies):\n",
        "    # For simplicity, we start off with only the blank English Language class\n",
        "    # and no model or pre-defined pipeline loaded.\n",
        "    nlp = English()\n",
        "    if not companies:  # set default companies if none are set via args\n",
        "        companies = [\"Alphabet Inc.\", \"Google\", \"Netflix\", \"Apple\"]  # etc.\n",
        "    component = TechCompanyRecognizer(nlp, companies)  # initialise component\n",
        "    nlp.add_pipe(component, last=True)  # add last to the pipeline\n",
        "\n",
        "    doc = nlp(text)\n",
        "    print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
        "    print(\"Tokens\", [t.text for t in doc])  # company names from the list are merged\n",
        "    print(\"Doc has_tech_org\", doc._.has_tech_org)  # Doc contains tech orgs\n",
        "    print(\"Token 0 is_tech_org\", doc[0]._.is_tech_org)  # \"Alphabet Inc.\" is a tech org\n",
        "    print(\"Token 1 is_tech_org\", doc[1]._.is_tech_org)  # \"is\" is not\n",
        "    print(\"Entities\", [(e.text, e.label_) for e in doc.ents])  # all orgs are entities\n",
        "\n",
        "\n",
        "class TechCompanyRecognizer(object):\n",
        "    \"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
        "    based on list of single or multiple-word company names. Companies are\n",
        "    labelled as ORG and their spans are merged into one token. Additionally,\n",
        "    ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
        "    respectively.\"\"\"\n",
        "\n",
        "    name = \"tech_companies\"  # component name, will show up in the pipeline\n",
        "\n",
        "    def __init__(self, nlp, companies=tuple(), label=\"ORG\"):\n",
        "        \"\"\"Initialise the pipeline component. The shared nlpframework instance is used\n",
        "        to initialise the matcher with the shared vocab, get the label ID and\n",
        "        generate Doc objects as phrase match patterns.\n",
        "        \"\"\"\n",
        "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
        "\n",
        "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
        "        # so even if the list of companies is long, it's very efficient\n",
        "        patterns = [nlp(org) for org in companies]\n",
        "        self.matcher = PhraseMatcher(nlp.vocab)\n",
        "        self.matcher.add(\"TECH_ORGS\", None, *patterns)\n",
        "\n",
        "        # Register attribute on the Token. We'll be overwriting this based on\n",
        "        # the matches, so we're only setting a default value, not a getter.\n",
        "        Token.set_extension(\"is_tech_org\", default=False)\n",
        "\n",
        "        # Register attributes on Doc and Span via a getter that checks if one of\n",
        "        # the contained tokens is set to is_tech_org == True.\n",
        "        Doc.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
        "        Span.set_extension(\"has_tech_org\", getter=self.has_tech_org)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
        "        are found. Return the Doc, so it can be processed by the next component\n",
        "        in the pipeline, if available.\n",
        "        \"\"\"\n",
        "        matches = self.matcher(doc)\n",
        "        spans = []  # keep the spans for later so we can merge them afterwards\n",
        "        for _, start, end in matches:\n",
        "            # Generate Span representing the entity & set label\n",
        "            entity = Span(doc, start, end, label=self.label)\n",
        "            spans.append(entity)\n",
        "            # Set custom attribute on each token of the entity\n",
        "            for token in entity:\n",
        "                token._.set(\"is_tech_org\", True)\n",
        "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
        "            doc.ents = list(doc.ents) + [entity]\n",
        "        for span in spans:\n",
        "            # Iterate over all spans and merge them into one token. This is done\n",
        "            # after setting the entities – otherwise, it would cause mismatched\n",
        "            # indices!\n",
        "            span.merge()\n",
        "        return doc  # don't forget to return the Doc!\n",
        "\n",
        "    def has_tech_org(self, tokens):\n",
        "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
        "        is a tech org. Since the getter is only called when we access the\n",
        "        attribute, we can refer to the Token's 'is_tech_org' attribute here,\n",
        "        which is already set in the processing step.\"\"\"\n",
        "        return any([t._.get(\"is_tech_org\") for t in tokens])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)\n",
        "\n",
        "    # Expected output:\n",
        "    # Pipeline ['tech_companies']\n",
        "    # Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
        "    # Doc has_tech_org True\n",
        "    # Token 0 is_tech_org True\n",
        "    # Token 1 is_tech_org False\n",
        "    # Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline ['tech_companies']\n",
            "Tokens ['/root/.local', '/', 'share', '/', 'jupyter', '/', 'runtime', '/', 'kernel', '-', 'f0b77ba8', '-', '9143', '-', '4716', '-', '8052', '-', '2cdca3c67205.json']\n",
            "Doc has_tech_org False\n",
            "Token 0 is_tech_org False\n",
            "Token 1 is_tech_org False\n",
            "Entities []\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}