{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "45_SVO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/45_SVO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBLtSz8LWatj",
        "outputId": "be733362-afd7-4950-c56b-e3ef84a23082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Copyright 2017 Peter de Vocht\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.examples import sentences\n",
        "import en_core_web_sm\n",
        "\n",
        "# use spacy small model\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "# dependency markers for subjects\n",
        "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
        "# dependency markers for objects\n",
        "OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
        "# POS tags that will break adjoining items\n",
        "BREAKER_POS = {\"CCONJ\", \"VERB\"}\n",
        "# words that are negations\n",
        "NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
        "\n",
        "\n",
        "# does dependency set contain any coordinating conjunctions?\n",
        "def contains_conj(depSet):\n",
        "    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n",
        "           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n",
        "\n",
        "\n",
        "# get subs joined by conjunctions\n",
        "def _get_subs_from_conjunctions(subs):\n",
        "    more_subs = []\n",
        "    for sub in subs:\n",
        "        # rights is a generator\n",
        "        rights = list(sub.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if contains_conj(rightDeps):\n",
        "            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(more_subs) > 0:\n",
        "                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n",
        "    return more_subs\n",
        "\n",
        "\n",
        "# get objects joined by conjunctions\n",
        "def _get_objs_from_conjunctions(objs):\n",
        "    more_objs = []\n",
        "    for obj in objs:\n",
        "        # rights is a generator\n",
        "        rights = list(obj.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if contains_conj(rightDeps):\n",
        "            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(more_objs) > 0:\n",
        "                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n",
        "    return more_objs\n",
        "\n",
        "\n",
        "# find sub dependencies\n",
        "def _find_subs(tok):\n",
        "    head = tok.head\n",
        "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
        "        head = head.head\n",
        "    if head.pos_ == \"VERB\":\n",
        "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
        "        if len(subs) > 0:\n",
        "            verb_negated = _is_negated(head)\n",
        "            subs.extend(_get_subs_from_conjunctions(subs))\n",
        "            return subs, verb_negated\n",
        "        elif head.head != head:\n",
        "            return _find_subs(head)\n",
        "    elif head.pos_ == \"NOUN\":\n",
        "        return [head], _is_negated(tok)\n",
        "    return [], False\n",
        "\n",
        "\n",
        "# is the tok set's left or right negated?\n",
        "def _is_negated(tok):\n",
        "    parts = list(tok.lefts) + list(tok.rights)\n",
        "    for dep in parts:\n",
        "        if dep.lower_ in NEGATIONS:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# get all the verbs on tokens with negation marker\n",
        "def _find_svs(tokens):\n",
        "    svs = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = _get_all_subs(v)\n",
        "        if len(subs) > 0:\n",
        "            for sub in subs:\n",
        "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
        "    return svs\n",
        "\n",
        "\n",
        "# get grammatical objects for a given set of dependencies (including passive sentences)\n",
        "def _get_objs_from_prepositions(deps, is_pas):\n",
        "    objs = []\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
        "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or\n",
        "                         (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n",
        "                         (is_pas and tok.dep_ == 'pobj')])\n",
        "    return objs\n",
        "\n",
        "\n",
        "# get objects from the dependencies using the attribute dependency\n",
        "def _get_objs_from_attrs(deps, is_pas):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
        "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
        "            if len(verbs) > 0:\n",
        "                for v in verbs:\n",
        "                    rights = list(v.rights)\n",
        "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "                    if len(objs) > 0:\n",
        "                        return v, objs\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# xcomp; open complement - verb has no suject\n",
        "def _get_obj_from_xcomp(deps, is_pas):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
        "            v = dep\n",
        "            rights = list(v.rights)\n",
        "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "            if len(objs) > 0:\n",
        "                return v, objs\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# get all functional subjects adjacent to the verb passed in\n",
        "def _get_all_subs(v):\n",
        "    verb_negated = _is_negated(v)\n",
        "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
        "    if len(subs) > 0:\n",
        "        subs.extend(_get_subs_from_conjunctions(subs))\n",
        "    else:\n",
        "        foundSubs, verb_negated = _find_subs(v)\n",
        "        subs.extend(foundSubs)\n",
        "    return subs, verb_negated\n",
        "\n",
        "\n",
        "# is the token a verb?  (excluding auxiliary verbs)\n",
        "def _is_non_aux_verb(tok):\n",
        "    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n",
        "\n",
        "\n",
        "# return the verb to the right of this verb in a CCONJ relationship if applicable\n",
        "# returns a tuple, first part True|False and second part the modified verb if True\n",
        "def _right_of_verb_is_conj_verb(v):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "\n",
        "    # VERB CCONJ VERB (e.g. he beat and hurt me)\n",
        "    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n",
        "        for tok in rights[1:]:\n",
        "            if _is_non_aux_verb(tok):\n",
        "                return True, tok\n",
        "\n",
        "    return False, v\n",
        "\n",
        "\n",
        "# get all objects for an active/passive sentence\n",
        "def _get_all_objs(v, is_pas):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "\n",
        "    objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n",
        "    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "\n",
        "    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n",
        "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "    #    objs.extend(potentialNewObjs)\n",
        "    #    v = potentialNewVerb\n",
        "\n",
        "    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n",
        "    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n",
        "        objs.extend(potential_new_objs)\n",
        "        v = potential_new_verb\n",
        "    if len(objs) > 0:\n",
        "        objs.extend(_get_objs_from_conjunctions(objs))\n",
        "    return v, objs\n",
        "\n",
        "\n",
        "# return true if the sentence is passive - at he moment a sentence is assumed passive if it has an auxpass verb\n",
        "def _is_passive(tokens):\n",
        "    for tok in tokens:\n",
        "        if tok.dep_ == \"auxpass\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# resolve a 'that' where/if appropriate\n",
        "def _get_that_resolution(toks):\n",
        "    for tok in toks:\n",
        "        if 'that' in [t.orth_ for t in tok.lefts]:\n",
        "            return tok.head\n",
        "    return toks\n",
        "\n",
        "\n",
        "# simple stemmer using lemmas\n",
        "def _get_lemma(word):\n",
        "    tokens = nlp(word)\n",
        "    if len(tokens) == 1:\n",
        "        return tokens[0].lemma_\n",
        "    return word\n",
        "\n",
        "\n",
        "# print information for displaying all kinds of things of the parse tree\n",
        "def printDeps(toks):\n",
        "    for tok in toks:\n",
        "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
        "\n",
        "\n",
        "# expand an obj / subj np using its chunk\n",
        "def expand(item, tokens, visited):\n",
        "    if item.lower_ == 'that':\n",
        "        item = _get_that_resolution(tokens)\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    if hasattr(item, 'lefts'):\n",
        "        for part in item.lefts:\n",
        "            if part.pos_ in BREAKER_POS:\n",
        "                break\n",
        "            if not part.lower_ in NEGATIONS:\n",
        "                parts.append(part)\n",
        "\n",
        "    parts.append(item)\n",
        "\n",
        "    if hasattr(item, 'rights'):\n",
        "        for part in item.rights:\n",
        "            if part.pos_ in BREAKER_POS:\n",
        "                break\n",
        "            if not part.lower_ in NEGATIONS:\n",
        "                parts.append(part)\n",
        "\n",
        "    if hasattr(parts[-1], 'rights'):\n",
        "        for item2 in parts[-1].rights:\n",
        "            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n",
        "                if item2.i not in visited:\n",
        "                    visited.add(item2.i)\n",
        "                    parts.extend(expand(item2, tokens, visited))\n",
        "            break\n",
        "\n",
        "    return parts\n",
        "\n",
        "\n",
        "# convert a list of tokens to a string\n",
        "def to_str(tokens):\n",
        "    return ' '.join([item.text for item in tokens])\n",
        "\n",
        "\n",
        "# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n",
        "def findSVOs(tokens):\n",
        "    svos = []\n",
        "    is_pas = _is_passive(tokens)\n",
        "    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)]\n",
        "    visited = set()  # recursion detection\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = _get_all_subs(v)\n",
        "        # hopefully there are subs, if not, don't examine this verb any longer\n",
        "        if len(subs) > 0:\n",
        "            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n",
        "            if isConjVerb:\n",
        "                v2, objs = _get_all_objs(conjV, is_pas)\n",
        "                for sub in subs:\n",
        "                    for obj in objs:\n",
        "                        objNegated = _is_negated(obj)\n",
        "                        if is_pas:  # reverse object / subject for passive\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                        else:\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n",
        "            else:\n",
        "                v, objs = _get_all_objs(v, is_pas)\n",
        "                for sub in subs:\n",
        "                    for obj in objs:\n",
        "                        objNegated = _is_negated(obj)\n",
        "                        if is_pas:  # reverse object / subject for passive\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                        else:\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
        "    return svos\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "#doc = nlpframework(sentence[0])\n",
        "\n",
        "text = \"\"\"\n",
        "If some files are missing, a search is initiated, otherwise the files can be physically tracked to the intended location.\n",
        "\"\"\"\n",
        "doc = nlp(text)\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "findSVOs(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "If some files are missing, a search is initiated, otherwise the files can be physically tracked to the intended location.\n",
            "\n",
            "\n",
            " SPACE \n",
            "If SCONJ mark\n",
            "some DET det\n",
            "files NOUN nsubj\n",
            "are AUX aux\n",
            "missing VERB advcl\n",
            ", PUNCT punct\n",
            "a DET det\n",
            "search NOUN nsubjpass\n",
            "is AUX auxpass\n",
            "initiated VERB ccomp\n",
            ", PUNCT punct\n",
            "otherwise ADV advmod\n",
            "the DET det\n",
            "files NOUN nsubjpass\n",
            "can VERB aux\n",
            "be AUX auxpass\n",
            "physically ADV advmod\n",
            "tracked VERB ROOT\n",
            "to ADP prep\n",
            "the DET det\n",
            "intended ADJ amod\n",
            "location NOUN pobj\n",
            ". PUNCT punct\n",
            "\n",
            " SPACE \n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the intended location', 'track', 'the files')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}