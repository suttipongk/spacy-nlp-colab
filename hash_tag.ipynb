{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18_hash_tag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/18_hash_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FNaoPYNFpyH",
        "outputId": "9f24e81a-fc67-4c14-d2d1-f46aa8f0c021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.language import Language\n",
        "from spacy.tokenizer import Tokenizer, _get_regex_pattern\n",
        "from spacy.tokens import Token\n",
        "from spacy.lang.tokenizer_exceptions import URL_PATTERN\n",
        "\n",
        "def firstSolutionReplace():\n",
        "    sentence = \"This is my twitter update #MyTopic\"\n",
        "    parsed = nlp(sentence)\n",
        "    print([token.text for token in parsed])\n",
        "\n",
        "    new_sentence = re.sub(r'#(\\w+)',r'ZZZPLACEHOLDERZZZ\\1',sentence)\n",
        "    print(new_sentence)\n",
        "    #['This', 'is', 'my', 'twitter', 'update', 'ZZZPLACEHOLDERZZZMyTopic']\n",
        "\n",
        "    parsed = nlp(new_sentence)\n",
        "    print([token.text for token in parsed])\n",
        "    print([x.replace('ZZZPLACEHOLDERZZZ','#') for x in [token.text for token in parsed]])\n",
        "    #['This', 'is', 'my', 'twitter', 'update', '#MyTopic']\n",
        "\n",
        "def secondSolutionParse():\n",
        "    my_str = \"Tweet hashtags #MyHashOne #MyHashTwo\"\n",
        "    parsed = nlp(my_str)\n",
        "\n",
        "    print([(x.text,x.pos_) for x in parsed])\n",
        "    #[('Tweet', 'PROPN'), ('hashtags', 'NOUN'), ('#', 'NOUN'), ('MyHashOne', 'NOUN'), ('#', 'NOUN'), ('MyHashTwo', 'PROPN')]\n",
        "\n",
        "    indexes = [m.span() for m in re.finditer('#\\w+',my_str,flags=re.IGNORECASE)]\n",
        "    print(indexes)\n",
        "    #[(15, 25), (26, 36)]\n",
        "\n",
        "    for start,end in indexes:\n",
        "        parsed.merge(start_idx=start,end_idx=end)\n",
        "\n",
        "    print([(x.text,x.pos_) for x in parsed])\n",
        "    #[('Tweet', 'PROPN'), ('hashtags', 'NOUN'), ('#MyHashOne', 'NOUN'), ('#MyHashTwo', 'PROPN')]\n",
        "\n",
        "def thirdSolutionMatcher():\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
        "\n",
        "    doc = nlp(\"This is a #sentence. Here is another #hashtag. #The #End.\")\n",
        "    matches = matcher(doc)\n",
        "    hashtags = []\n",
        "    for match_id, start, end in matches:\n",
        "        hashtags.append(doc[start:end])\n",
        "\n",
        "    for span in hashtags:\n",
        "        span.merge()\n",
        "\n",
        "    print([t.text for t in doc])\n",
        "\n",
        "def hashtag_pipe(doc):\n",
        "    merged_hashtag = False\n",
        "    while True:\n",
        "        for token_index,token in enumerate(doc):\n",
        "            if token.text == '#':\n",
        "                if token.head is not None:\n",
        "                    start_index = token.idx\n",
        "                    end_index = start_index + len(token.head.text) + 1\n",
        "                    if doc.merge(start_index, end_index) is not None:\n",
        "                        merged_hashtag = True\n",
        "                        break\n",
        "        if not merged_hashtag:\n",
        "            break\n",
        "        merged_hashtag = False\n",
        "    return doc\n",
        "\n",
        "def create_tokenizer(nlp):\n",
        "    # spacy defaults: when the standard behaviour is required, they\n",
        "    # need to be included when subclassing the tokenizer\n",
        "    prefix_re = spacy.util.compile_prefix_regex(Language.Defaults.prefixes)\n",
        "    infix_re = spacy.util.compile_infix_regex(Language.Defaults.infixes)\n",
        "    suffix_re = spacy.util.compile_suffix_regex(Language.Defaults.suffixes)\n",
        "\n",
        "    # extending the default url regex with regex for hashtags with \"or\" = |\n",
        "    hashtag_pattern = r'''|^(#[\\w_-]+)$'''\n",
        "    url_and_hashtag = URL_PATTERN + hashtag_pattern\n",
        "    url_and_hashtag_re = re.compile(url_and_hashtag)\n",
        "\n",
        "    # set a custom extension to match if token is a hashtag\n",
        "    hashtag_getter = lambda token: token.text.startswith('#')\n",
        "    Token.set_extension('is_hashtag', getter=hashtag_getter)\n",
        "\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
        "                     suffix_search=suffix_re.search,\n",
        "                     infix_finditer=infix_re.finditer,\n",
        "                     token_match=url_and_hashtag_re.match\n",
        "                     )\n",
        "\n",
        "def fiveSolution():\n",
        "    nlp = spacy.load('en')\n",
        "\n",
        "    # get default pattern for tokens that don't get split\n",
        "    re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
        "    # add your patterns (here: hashtags and in-word hyphens)\n",
        "    re_token_match = \"({re_token_match}|#\\w+|\\w+-\\w+)\"\n",
        "\n",
        "    # overwrite token_match function of the tokenizer\n",
        "    nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
        "\n",
        "    text = \"@Pete: choose low-carb #food #eatsmart ;-) üòãüëç\"\n",
        "    doc = nlp(text)\n",
        "    print(doc)\n",
        "\n",
        "#https://stackoverflow.com/questions/43388476/how-could-spacy-tokenize-hashtag-as-a-whole\n",
        "if __name__ == \"__main__\":\n",
        "    nlp = spacy.load('en')\n",
        "    nlp.add_pipe(hashtag_pipe)\n",
        "\n",
        "    doc = nlp(\"twitter #hashtag\")\n",
        "    assert len(doc) == 2\n",
        "    assert doc[0].text == 'twitter'\n",
        "    assert doc[1].text == '#hashtag'\n",
        "\n",
        "    #======================================\n",
        "    nlp.tokenizer = create_tokenizer(nlp)\n",
        "    doc = nlp(\"#spreadhappiness #smilemore so_great@good.com https://www.somedomain.com/foo\")\n",
        "\n",
        "    for token in doc:\n",
        "        print(token.text)\n",
        "        if token._.is_hashtag:\n",
        "            print(\"-> matches hashtag\")\n",
        "    ## returns: \"#spreadhappiness -> matches hashtag #smilemore -> matches hashtag so_great@good.com https://www.somedomain.com/foo\"\n",
        "    #======================================\n",
        "\n",
        "    fiveSolution()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#spreadhappiness\n",
            "-> matches hashtag\n",
            "#smilemore\n",
            "-> matches hashtag\n",
            "so_great@good.com\n",
            "https://www.somedomain.com/foo\n",
            "@Pete: choose low-carb #food #eatsmart ;-) üòãüëç\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}