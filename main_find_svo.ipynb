{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21_main_find_SVO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kullawattana/thesis_2020_spacy_colab/blob/master/21_main_find_SVO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9jIj_K5Pejl",
        "outputId": "5cab5158-c219-4f41-9a18-d6cfcf8a0cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, sys\n",
        "\n",
        "# NLP Processing\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n",
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
        "\n",
        "#---------------------------------Step 1 Import Data && step 2 custom sentence boundary && Step 3 custom pipeline---------------------------------\n",
        "def customSentenceBoundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == \"otherwise\":\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "        if token.text == \"and\":\n",
        "            doc[token.i].is_sent_start = True\n",
        "        if token.text == \"...\":\n",
        "            doc[token.i].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "#---------------------------------step 4 find SVO---------------------------------\n",
        "def getSubsFromConjunctions(subs):\n",
        "    moreSubs = []\n",
        "    for sub in subs:\n",
        "        # rights is a generator\n",
        "        rights = list(sub.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreSubs) > 0:\n",
        "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
        "    return moreSubs\n",
        "\n",
        "def findSubs(tok):\n",
        "    head = tok.head\n",
        "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
        "        head = head.head\n",
        "    if head.pos_ == \"VERB\":\n",
        "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
        "        if len(subs) > 0:\n",
        "            verbNegated = isNegated(head)\n",
        "            subs.extend(getSubsFromConjunctions(subs))\n",
        "            return subs, verbNegated\n",
        "        elif head.head != head:\n",
        "            return findSubs(head)\n",
        "    elif head.pos_ == \"NOUN\":\n",
        "        return [head], isNegated(tok)\n",
        "    return [], False\n",
        "\n",
        "def getAllSubs(v):\n",
        "    verbNegated = isNegated(v)\n",
        "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
        "    if len(subs) > 0:\n",
        "        subs.extend(getSubsFromConjunctions(subs))\n",
        "    else:\n",
        "        foundSubs, verbNegated = findSubs(v)\n",
        "        subs.extend(foundSubs)\n",
        "    return subs, verbNegated\n",
        "\n",
        "def isNegated(tok):\n",
        "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
        "    for dep in list(tok.lefts) + list(tok.rights):\n",
        "        if dep.lower_ in negations:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def findSVOs(tokens):\n",
        "    svos = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
        "\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = getAllSubs(v)\n",
        "        if len(subs) > 0:\n",
        "            v, objs = getAllObjs(v)\n",
        "\n",
        "            for sub in subs:\n",
        "                for obj in objs:\n",
        "                    objNegated = isNegated(obj)\n",
        "                    svos.append((sub.lower_, \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, obj.lower_))\n",
        "    return svos\n",
        "\n",
        "def getAllObjs(v):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "    objs.extend(getObjsFromPrepositions(rights))\n",
        "\n",
        "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
        "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "        objs.extend(potentialNewObjs)\n",
        "        v = potentialNewVerb\n",
        "    if len(objs) > 0:\n",
        "        objs.extend(getObjsFromConjunctions(objs))\n",
        "    return v, objs\n",
        "\n",
        "def getObjsFromPrepositions(deps):\n",
        "    objs = []\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
        "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
        "    return objs\n",
        "\n",
        "def getObjFromXComp(deps):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
        "            v = dep\n",
        "            rights = list(v.rights)\n",
        "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "            objs.extend(getObjsFromPrepositions(rights))\n",
        "            if len(objs) > 0:\n",
        "                return v, objs\n",
        "    return None, None\n",
        "\n",
        "def getObjsFromConjunctions(objs):\n",
        "    moreObjs = []\n",
        "    for obj in objs:\n",
        "        # rights is a generator\n",
        "        rights = list(obj.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreObjs) > 0:\n",
        "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
        "    return moreObjs\n",
        "\n",
        "def printDeps(toks):\n",
        "    for tok in toks:\n",
        "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
        "\n",
        "#---------------------------------step 5 match pattern-----------------------------------\n",
        "def match_pattern(text, patterns):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add('Boots', None, patterns)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match in matches:\n",
        "        match_id, start, end = match\n",
        "        span = doc[start:end]\n",
        "        print(match, span.text)\n",
        "\n",
        "def check_subject_pattern(text):\n",
        "    #find subject\n",
        "    patterns = [\n",
        "        {'POS': 'DET'},\n",
        "        {'POS': 'NOUN'}\n",
        "    ]\n",
        "    match_pattern(text, patterns)\n",
        "\n",
        "def check_subject_propn_pattern(text):\n",
        "    #find subject\n",
        "    patterns = [\n",
        "        {'POS': 'DET'},\n",
        "        {'POS': 'PROPN'}\n",
        "    ]\n",
        "    match_pattern(text, patterns)\n",
        "\n",
        "def check_object_pattern(text):\n",
        "    #find object\n",
        "    patterns = [\n",
        "        {'POS': 'ADJ'},\n",
        "        {'POS': 'NOUN'}\n",
        "    ]\n",
        "    match_pattern(text, patterns)\n",
        "\n",
        "def check_if_then_pattern(text):\n",
        "    # IF-THEN, ...\n",
        "    patterns = [\n",
        "        {'POS': 'DET'},\n",
        "        {'POS': 'NOUN'},\n",
        "        {'POS': 'VERB'}\n",
        "    ]\n",
        "    match_pattern(text, patterns)\n",
        "\n",
        "# Finding\n",
        "def finding_subject_dependency(text):\n",
        "    for possible_subject in doc:\n",
        "        if possible_subject.dep == nsubj:\n",
        "            check_subject_pattern(text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"this is a book.\"\n",
        "    print(text)\n",
        "    #--------------Step 1 Import----------------\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "    #--------------Step 2 Custom Boundary && Step 3 Adding Pipeline Custom Boundary----------------\n",
        "    nlp.add_pipe(customSentenceBoundaries, before=\"parser\")\n",
        "    doc = nlp(text)\n",
        "    print(\"After:\", [sent.text for sent in doc.sents])\n",
        "    #--------------Step 4 Find SVO-------------------\n",
        "    sentences = list(doc.sents)\n",
        "    for i, sent in enumerate(doc.sents):\n",
        "        print(sent)         #print(i, \"a\", sent, type(sent))\n",
        "        #step 4\n",
        "        svos = findSVOs(sent)\n",
        "        print(svos)\n",
        "        #----------------Step 5 match pattern by Sentence----------------------\n",
        "        finding_subject_dependency(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is a book.\n",
            "Before: ['this is a book.']\n",
            "After: ['this is a book.']\n",
            "this is a book.\n",
            "[]\n",
            "(18231591219755621867, 2, 4) a book\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}